---
title: "PROJECT: Porewater Nutrients"
author: "Month YEAR Samples"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
    latex_engine: xelatex
output_dir: "To Be Reviewed/PDF"

---

\newpage

## Run Information 
```{r run information, include=TRUE}
cat("Run Information: NAME ") #lets you know what section you're in 
#set the run date & user name 
  sample_year <- "2024"
  user <- "NAME"

#identify the files you want to read in 
  #read in as a list to accommadate ultiple runs in a month
  NOx_files <- c("Raw Data/20260120_Tempest2024_NH3_PO4_Run1.csv")
  NH3_PO4_files <- c("Raw Data/20260209_Tempest2024_VNOx_Run1.csv")

# Define the file path for QAQC log file - NO Need to change just check year 
  file_path <- "Raw Data/SEAL_TEMPEST_QAQC_Log_2024.csv"
  final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202407.csv"

#record any notes about the run or anything other info here: 
  run_notes <- "" 
  
#Set up file path for metadata 
  #downloaded metadata csv - downloaded from Google drive as csv for this year
  Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2024.csv"

  cat(run_notes)
```

##Setup
```{r setup, include=FALSE}

#let you know which section you are in 
cat("Setup")

#a link to the Gitbook or whatever protocol you are using for this analysis 


#load/install packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  dplyr,
  purrr,
  ggplot2,
  ggpubr,
  tidyr,
  tidyverse,
  lubridate,
  tinytex,
  stringr,
  writexl,
  readr,
  readxl,
  purrr,
  tinytex,
  broom,
  data.table,
  raster,
  googledrive)

#Coefficients / constants that are needed for calculations 
N_mw <- 14.0067    # molecular weight of N 
P_mw <- 30.973762  # molecular weight of P 
Con1 <- 1000       # conversion factor value
Con2 <- 1000000    # conversion factor value 

#Detection limit and top standards for flagging 
NOx_dl <- 0.025  #mgL
NH3_dl <- 0.02  #mgL
PO4_dl <- 0.003 #mgL

NOx_top <- 1  #mgL
NH3_top <- 2  #mgL
PO4_top <- 0.3 #mgL

#Check Standard concnetrations 
NOx_CCV <- 0.5 
NH3_CCV <- 1.0 
PO4_CCV <- 0.15 

#Spike Concentrations 
NOx_Spk <- 0.5 #ppm or mg/L
NH3_Spk <- 1
PO4_Spk <- 0.152 

#expected ranges for sample concentrations used for flags 
r2_cutoff = 0.990
chk_flag = 0.25
chk_conc_flag = 15 #cutoff level for percent difference of check stds vs. expected concentration 
chks_flag = 60
rep_flag = 25 
#^this is a 25% error between samples
#blank_flag is calculated based on samples later in this code

#pe check Concentrations
NH3_pe <- 1.034
NOx_pe <- 1.51
PO4_pe <- 0.824

```

##Pull in active porewater tracking inventory sheet from Google Drive: 
```{r rest of metadata, include=FALSE}

# #Run these to pull the TEMPEST porewater metadata into GitHub if not already there
 # inventory_directory <- "https://docs.google.com/spreadsheets/d/1sFWq-WKhemPzbOFInqhCu_Lx0lsO6a_Z/edit#gid=496164093"
 # 
 # drive_download(inventory_directory, overwrite = TRUE)
 # 
 # sheet_names <- excel_sheets("TEMPEST_PorewaterInventory_May2022_Present.xlsx")
 # sheet_names

raw_metadata_lys <- read_excel("TEMPEST_PorewaterInventory_May2022_Present.xlsx", sheet = "Porewater - Individual", skip = 3)

raw_metadata_sw <- read_excel("TEMPEST_PorewaterInventory_May2022_Present.xlsx", sheet = "Source Water", skip = 3)


```


##Create similar sample IDs to match with run samples 
```{r pull in metadata for later, include=FALSE}

#select SO4/Cl samples 
raw_metadata_lys <- subset(raw_metadata_lys, Analyte == "NUTR")

#select 2024 samples
raw_metadata_lys_year <- raw_metadata_lys %>%  
  filter(str_detect(Sample_ID, sample_year))

#select event samples 
raw_metadata_lys_event <- raw_metadata_lys_year %>%  
  filter(str_detect(Sample_ID, "_T"))

#separate event samples into columns
raw_metadata_lys_event <- raw_metadata_lys_event %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Event_Time", "Collection_Date", "Time_of_day"),
    remove = FALSE)

#select non-event samples 
raw_metadata_lys_monmon1 <- raw_metadata_lys_year %>%  
  filter(!str_detect(Sample_ID, "_T"))

#separate non-event samples into columns
raw_metadata_lys_monmon <- raw_metadata_lys_monmon1 %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Event_Time", "Time_of_day"), #Warning is okay
    remove = FALSE)

#combine event and non-event samples 
raw_metadata_lys_combined <- rbind(raw_metadata_lys_event, raw_metadata_lys_monmon)
raw_metadata_lys_combined <- raw_metadata_lys_combined %>%
  dplyr::select("Sample_ID", "Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Evacuation_date_YYYMMDD", "Event_Time", "Time_of_day", "Volume_mL", "Notes")


#create IDs from what was collected for comparison later
raw_metadata_lys_combined <- raw_metadata_lys_combined %>%
  mutate(NUTR_ID = paste(Project,
                          Zone,
                          Grid,
                          Event_Time, 
                          Collection_Date, 
                          sep = "_"))



#select SO4/Cl samples 
raw_metadata_sw <- raw_metadata_sw %>%  
  filter(str_detect(Sample_ID, "NUTR"))

#select 2024 samples
raw_metadata_sw_year <- raw_metadata_sw %>%  
  filter(str_detect(Sample_ID, sample_year))

#select rainwater samples 
raw_metadata_sw_rw <- raw_metadata_sw_year %>%  
  filter(str_detect(Sample_ID, "_Rainwater"))

#separate event samples into columns
raw_metadata_sw_rw <- raw_metadata_sw_rw %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", NA, "Source", "Zone", "Analyte", "Collection_Date", "Grid", "Depth", "Event_Time", "Time_of_day"),
    remove = FALSE)

#select sourcewater samples 
raw_metadata_sw_sw <- raw_metadata_sw_year %>%  
  filter(!str_detect(Sample_ID, "_Rainwater"))

#separate samples into columns
raw_metadata_sw_sw <- raw_metadata_sw_sw %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Source", "Zone", "Analyte", "Collection_Date", "Time_of_day", "Grid", "Depth", "Event_Time"),
    remove = FALSE)

#combine rw and sw samples 
raw_metadata_sw_combined <- rbind(raw_metadata_sw_rw, raw_metadata_sw_sw)
raw_metadata_sw_combined <- raw_metadata_sw_combined %>%
  rename("Notes" = "Notes:") %>%
  dplyr::select("Sample_ID", "Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Event_Time", "Time_of_day", "Volume_mL", "Notes")

#change "Source" to "Sourcewater" 
raw_metadata_sw_combined$Source <- replace(raw_metadata_sw_combined$Source, raw_metadata_sw_combined$Source == "Source", "SourceWater")

#create IDs from what was collected for comparison later
raw_metadata_sw_combined <- raw_metadata_sw_combined %>%
  mutate(NUTR_ID = paste(Project,
                          Zone,
                          Source, 
                          Collection_Date, 
                          sep = "_"))


#combine porewater and sourcewater samples
nutr_metadata <- bind_rows(raw_metadata_lys_combined, raw_metadata_sw_combined)

#remove old ID's
nutr_metadata$Sample_ID <- NULL

#Change TMP to TEMPEST in sample IDs
nutr_metadata$NUTR_ID <- gsub("TMP", "TEMPEST", nutr_metadata$NUTR_ID)

#Remove NAs from sample IDs
nutr_metadata$NUTR_ID <- gsub("_NA_", "_", nutr_metadata$NUTR_ID)


# head(nutr_metadata)

```


##Import Data & Clean
```{r Import Data, include=FALSE}
cat("Import Data")

#set file path for data - in run info chunk 
path <- ("file path")

#Read in Raw Data 
data_NOx <- map(NOx_files, read.csv)
data_NH3_PO4 <- map(NH3_PO4_files, read.csv)

# Combining Files
df_combo <- bind_rows(data_NOx, data_NH3_PO4)

# Rename columns
df1 <- df_combo %>%
  dplyr::select(Sample_Name = 4,
         Run_Number = 5,
         Conc = 6,
         Absorbance = 7,
         Dilution = 9,
         Unit = 12,
         Test = 13,
         Run_Time = 15) %>%
  mutate(Run_Number = as.numeric(Run_Number))

#Checking column headers
head(df)

df_all <- df1 %>% mutate(Run_Number = as.numeric(Run_Number),
         Run_Date = format(as.POSIXct(Run_Time,format='%m/%d/%Y %H:%M'),format='%m/%d/%Y'))

#Initialize Flags
DF_Flags <- df_all %>% dplyr::select(Run_Date) %>% distinct() %>% na.omit()

```

\newpage

## Assessing standard Curves
```{r Pull out Standard Curves, echo=FALSE}

#Inputting Standard Values Based on Protocol (LINK PROTOCOL)
split_data <- df_all %>% dplyr::select(Sample_Name, Conc, Absorbance, Test, Run_Date) %>% filter(Sample_Name %ilike% "Standard") %>%
  mutate(Conc = ifelse(Test == "Ammonia 2", 
                       case_when(
    Sample_Name == "Standard 0" ~  0.0,
    Sample_Name == "Standard 1" ~  0.0,
    Sample_Name == "Standard .0389" ~ 0.0389,
    Sample_Name == "Standard .1000" ~ 0.1,
    Sample_Name == "Standard .2000" ~ 0.2,
    Sample_Name == "Standard .5000" ~ 0.5,
    Sample_Name == "Standard 1.0000" ~ 1.0,
    Sample_Name == "Standard 1.5000" ~ 1.5,
    Sample_Name == "Standard 2.0000" ~ 2),
    ifelse(Test == "o-PHOS 0.3", 
           case_when(
    Sample_Name == "Standard 0" ~  0.0,
    Sample_Name == "Standard 1" ~  0.0,
    Sample_Name == "Standard .006" ~ 0.0060,
    Sample_Name == "Standard .0150" ~ 0.0150,
    Sample_Name == "Standard .03" ~ 0.0300,
    Sample_Name == "Standard .075" ~ 0.0750,
    Sample_Name == "Standard .15" ~ 0.1500,
    Sample_Name == "Standard .225" ~ 0.2250,
    Sample_Name == "Standard .3" ~ 0.3000),
    ifelse(Test == "Vanadium NOx",
           case_when(
    Sample_Name == "Standard 0" ~  0.0,
    Sample_Name == "Standard 1" ~  0.0,
    Sample_Name == "Standard 90" ~ 0.0222,
    Sample_Name == "Standard 91" ~ 0.05,
    Sample_Name == "Standard 92" ~ 0.1,
    Sample_Name == "Standard 93" ~ 0.25,
    Sample_Name == "Standard 94" ~ 0.5,
    Sample_Name == "Standard 95" ~ 0.75,
    Sample_Name == "Standard 96" ~ 1.0), 
    "TEST NOT FOUND")))) %>%
  group_by(Run_Date, Test) %>% 
  na.omit() %>%
  group_split() 

#run regression and get coefficients by Run
 
Slope_raw_list <- map(
  split_data,
  ~ .x %>% mutate(
    R2 = ifelse(
      Test != "Vanadium NOx",
      summary(lm(Conc ~ Absorbance, data = .))$adj.r.squared,
      summary(lm(Conc ~ Absorbance + I(Absorbance^2), data = .))$adj.r.squared
    ) ,
    Slope = ifelse(
      Test != "Vanadium NOx",
      coef(lm( Conc ~ Absorbance, data = . ))[2], coef(lm(Conc ~ Absorbance + I(Absorbance^2), data = . ))[2]
      ),
    Intercept = ifelse(
      Test != "Vanadium NOx",
      coef(lm(Conc ~ Absorbance, data = . ))[1], 
      coef(lm(Conc ~ Absorbance + I(Absorbance^2), data = . ))[1]
      ),
    Quadratic = ifelse(
      Test == "Vanadium NOx",
      coef(lm( Conc ~ Absorbance + I(Absorbance^2), data = .))[3] , NA
      )
      )
  )



 Slope_raw <- bind_rows(Slope_raw_list)
  
 #check R2 and flag
 Slope_Std_Crvs <- Slope_raw %>% 
   dplyr::select(Run_Date, R2, Slope, Intercept, Quadratic, Test) %>% 
   distinct() %>% 
   mutate(R2_Flag = ifelse(R2 >= r2_cutoff, "R2_Pass", "R2_Fail"))

 #join standard curve CVs and regression values
 
 R2_Check <- Slope_Std_Crvs %>% 
   dplyr::select(Run_Date,
                 R2,
                 Slope,
                 Intercept,
                 Quadratic,
                 Test,
                 R2_Flag) %>%
   distinct()
 
 For_crv_plot <- Slope_raw %>% mutate(Conc =  as.numeric(Conc))

 Curves_Plot <- ggplot(For_crv_plot, aes(Conc, Absorbance)) + 
   geom_point(size = 3, col = "black") +
  geom_smooth(data = subset(For_crv_plot, Test != "Vanadium NOx"), method = "lm", formula = "y~x",se = FALSE, col = "black") +
  geom_smooth(data = subset(For_crv_plot, Test == "Vanadium NOx"), method = "lm", formula = "y~x + I(x^2)",se = FALSE, col = "black") + 
   facet_wrap(Run_Date~Test, scales = "free") + 
   labs(title = "Standard Curves") + 
   theme_classic()
 
Curves_Plot
 

R2_Flags <- R2_Check %>% dplyr::select(Test, Run_Date, R2_Flag, R2) #make df for display 

R2_Flags_join <- R2_Flags %>% dplyr::select(Test, Run_Date, R2_Flag)

knitr::kable(R2_Flags) #display Regression flags and R2


```

## Update Standard Log
```{r Update Standard Curve Log, echo = FALSE}

#log <- tibble(Test = c(), Intercept= c(), Slope= c(), Quadratic= c(), Run_Date= c(), User= c()) #create standard log in proper format

old_log <- read.csv(file_path) %>% mutate(Run_Date = as.Date(as.character(Run_Date), tryFormats = c("%m/%d/%Y", "%Y-%m-%d"))) %>% mutate(Run = "Past Runs")#read in log and format date

add_log <- R2_Check %>% dplyr::select(Test, Run_Date, R2, Slope, Intercept, Quadratic) %>%
  mutate(Test = case_when(
    Test == "Ammonia 2" ~ "NH3",
    Test == "Vanadium NOx" ~ "NOx",
    Test == "o-PHOS 0.3" ~ "PO4"
  )) %>% mutate(Run_Date = as.Date(as.character(Run_Date), format = "%m/%d/%Y")) %>% mutate(User = user) %>% mutate(Run = "Current Runs")#Format new run data for log


new_log <- rbind(old_log, add_log) %>% mutate(across(where(is.numeric), \(x) round(x, digits = 4))) %>% distinct() #bind rows, round data, and remove possible duplicate data from rerunning code
Reg_Coef <- new_log %>% group_by(Test) %>% reframe(Mean_Slope = mean(Slope, na.rm = TRUE), SD_Slope = sd(Slope, na.rm = TRUE), Mean_Int = mean(Intercept, na.rm = TRUE), SD_Int = sd(Intercept, na.rm = TRUE), Mean_Quad = mean(Quadratic, na.rm = TRUE), SD_Quad = sd(Quadratic, na.rm = TRUE), Test = Test ) %>% distinct()

Coef_Lim <- tibble(Test      = c("NH3", "PO4", "NOx"),
                   Min_Slope = c(1.5,2.0,1.0 ),
                   Max_Slope = c(2.5,3.0,2.0 ),
                   Min_Int   = c(-0.06,-0.005 ,-0.2 ),
                   Max_Int   = c(0,0,0),
                   Min_Quad  = c(NA,NA,-0.2),
                   Max_Quad  = c(NA,NA,0),
                   Run_Date = median(new_log$Run_Date),
                   Run = "Past Runs")

#Plot Slope log
Slopes_chk <- ggplot(new_log, aes(x = Run_Date, y = Slope, col = Run)) +
  scale_color_manual(values = c("Past Runs" = "grey27", "Current Runs" = "cyan3")) +
  geom_abline(data = Reg_Coef, aes(intercept = Mean_Slope - 2*SD_Slope, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_abline(data = Reg_Coef, aes(intercept = Mean_Slope + 2*SD_Slope, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_point(size = 2) +
  geom_point(data = Coef_Lim, aes(y = Max_Slope), alpha = 0) +
  geom_point(data = Coef_Lim, aes(y = Min_Slope), alpha = 0) +
  facet_wrap(Test~., scales = "free", ncol = 1) +
  theme_classic()+
  ggtitle("Slope Log") 


#Plot Intercept log
ints_chk <- ggplot(new_log, aes(x = Run_Date, y = Intercept, col = Run)) +
  scale_color_manual(values = c("Past Runs" = "grey27", "Current Runs" = "cyan3")) +
  geom_abline(data = Reg_Coef, aes(intercept = Mean_Int - 2*SD_Int, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_abline(data = Reg_Coef, aes(intercept = Mean_Int + 2*SD_Int, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_point(size = 2) +
  geom_point(data = Coef_Lim, aes(y = Max_Int), alpha = 0) +
  geom_point(data = Coef_Lim, aes(y = Min_Int), alpha = 0) +
  facet_wrap(Test~., scales = "free", ncol = 1) +
  theme_classic()+
  ggtitle("Intercept Log")

NOx_Coef <- Reg_Coef %>% filter(Test == "NOx")
NOx_Lim <- Coef_Lim %>% filter(Test == "NOx")

NOx_Log <- new_log %>% filter(Test == "NOx")

#Plot Quadratic Coef log
quad_chk <- ggplot(NOx_Log, aes(x = Run_Date, y = Quadratic, col = Run)) +
  scale_color_manual(values = c("Past Runs" = "grey27", "Current Runs" = "cyan3")) +
  geom_abline(data = NOx_Coef, aes(intercept = Mean_Quad - 2*SD_Quad, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_abline(data = NOx_Coef, aes(intercept = Mean_Quad + 2*SD_Quad, slope = 0), linetype = "dashed", linewidth = 1.5, col = "red" )+
  geom_point(size = 2) +
  geom_point(data = NOx_Lim, aes(y = Max_Quad), alpha = 0) +
  geom_point(data = NOx_Lim, aes(y = Min_Quad), alpha = 0) +
  theme_classic() +
  ylab("Quadratic Coefficient")+
  ggtitle("NOx Quadratic Coef Log") 

Slopes_chk

quad_chk

ints_chk

write.csv(new_log, "Raw Data/SEAL_TEMPEST_QAQC_Log_2024.csv", row.names = FALSE) #write out log



```

 
 \newpage
 
## Dilution Corrections - ensure the latest dilution is kept
```{r Dilution Corrections, echo=FALSE}
df_all_cor <- df_all %>% na.omit() %>% group_by(Run_Number, Run_Date, Sample_Name, Test) %>%
  mutate(Keep = ifelse(Dilution == max(Dilution), "Keep", "Rmv")) %>% 
  filter(Keep == "Keep") %>% ungroup() %>% #keep only latest dilution
  mutate(Pair_ID = ifelse(
  grepl("Duplicate", Sample_Name, ignore.case = TRUE),
  paste0(lag(Sample_Name), "_", lag(Run_Number)),
  # pair only if same run
  paste0(Sample_Name, "_", Run_Number) #create pair ID
))

#Display duplicated samples
df_dup <- df_all %>% na.omit() %>% filter( #get rid of anything other than samples
  !Sample_Name %like% "Standard",
  !Sample_Name %like% "1mg",
  !Sample_Name %ilike% "pe",
  !Sample_Name %ilike% "ppt",
  !Sample_Name %like% "Blank",
  !Sample_Name %like% "Duplicate",
  !Sample_Name %like% "Spike",
  !Sample_Name %like% "0.15 mg",
  !Sample_Name %like% "Nitrate",
  !Sample_Name %like% "Nitrite",
  !Sample_Name %like% "CCV",
  !Sample_Name %like% "CCB"
) %>% group_by(Run_Number, Run_Date, Sample_Name, Test) %>% mutate(Duplicated = ifelse(length(Sample_Name) == 1, "No Duplicates", "Yes Duplicates")) %>% filter(Duplicated == "Yes Duplicates") %>% ungroup() #check if samples were duplicated and get those samples

df_dup_disp <- df_dup %>% dplyr::select(Sample_Name) %>% distinct() #get list of duplicated samples

df_dup_dil_samp <- df_dup %>% group_by(Run_Number, Run_Date, Sample_Name, Test) %>% mutate(Diluted = ifelse(length(unique(Dilution)) == 1, "No Dilution", "Yes Diluted")) %>% ungroup() # check if any samples were reran without dilution

df_dup_dil_disp <- df_dup_dil_samp %>% filter(Diluted == "Yes Diluted") %>% dplyr::select(Sample_Name) %>% distinct() #get list of diluted samples

df_dup_ndil_disp <- df_dup_dil_samp %>% filter(Diluted == "No Dilution") %>% dplyr::select(Sample_Name) %>% distinct() #get list of reran samples without dilutions

#Which Samples were ran multiple times and were diluted 
ifelse(
  length(df_dup_disp$Sample_Name) == 0,
  paste("No Reruns"),
  paste("Reran Samples:", df_dup_disp)
)

ifelse(
  length(df_dup_dil_disp$Sample_Name) == 0,
  paste("No Dilutions"),
  paste("Diluted Samples:", df_dup_disp)
)

ifelse(
  length(df_dup_ndil_disp$Sample_Name) == 0,
  paste("No Naming Issues Detected"),
  paste("Possible Naming Errors:", df_dup_disp)
)


```


## Performance Check
```{r pe Check, echo = FALSE}
NOx_peChk <- df_all_cor %>%
  filter(Test == "Vanadium NOx", #get only pe check for VNOx
         Pair_ID %ilike% "pe",
         !Pair_ID %ilike% "Tempest")  %>% group_by(Run_Date) %>%
  mutate(
    Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
    SD_Chk_Conc = sd(Conc, na.rm = TRUE),
    CV = cv(Conc),
    pct_diff_Chk = 100* abs(Mean_Chk_Conc - NOx_pe) / NOx_pe,
    NOx_pe_flag = ifelse(abs(pct_diff_Chk) < 25, "Performance Check Within 25% - PROCEED", "Performance Check NOT Within 25% - REASSESS")) %>% #flag data if percent difference is out of range
  dplyr::select(Pair_ID, Mean_Chk_Conc, Run_Date, pct_diff_Chk, Test, NOx_pe_flag) %>% distinct()
      

NH3_peChk <- df_all_cor %>%
  filter(Test == "Ammonia 2", #get only pe check for NH3
         Pair_ID %ilike% "pe",
         !Pair_ID %ilike% "Tempest")  %>% group_by(Run_Date) %>%
  mutate(
    Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
    SD_Chk_Conc = sd(Conc, na.rm = TRUE),
    CV = cv(Conc),
    pct_diff_Chk = 100 *abs(Mean_Chk_Conc - NH3_pe) / NH3_pe,
    NH3_pe_flag = ifelse(abs(pct_diff_Chk) < 25, "Performance Check Within 25% - PROCEED", "Performance Check NOT Within 25% - REASSESS")) %>% #flag data if percent difference is out of range
  dplyr::select(Pair_ID, Mean_Chk_Conc, Run_Date, pct_diff_Chk, Test, NH3_pe_flag) %>% distinct()

PO4_peChk <- df_all_cor %>%
  filter(Test == "o-PHOS 0.3", #get only pe check for PO4
         Pair_ID %ilike% "pe",
         !Pair_ID %ilike% "Tempest")  %>% group_by(Run_Date) %>%
  mutate(
    Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
    SD_Chk_Conc = sd(Conc, na.rm = TRUE),
    CV = cv(Conc),
    pct_diff_Chk = 100* abs(Mean_Chk_Conc - PO4_pe) / PO4_pe,
    PO4_pe_flag = ifelse(abs(pct_diff_Chk) < 25, "Performance Check Within 25% - PROCEED", "Performance Check NOT Within 25% - REASSESS")) %>% #flag data if percent difference is out of range
  dplyr::select(Pair_ID, Mean_Chk_Conc, Run_Date, pct_diff_Chk, Test, PO4_pe_flag) %>% distinct()
  
#Create Data Frames for Display
PE_PO4_flag_disp <- PO4_peChk %>% dplyr::select(Test, Run_Date, PO4_pe_flag, Mean_Chk_Conc) %>% rename(PE_Flag = PO4_pe_flag, PE_Conc = Mean_Chk_Conc) %>% mutate(PE_Target_Conc = PO4_pe) 

PE_NOx_flag_disp <- NOx_peChk %>% dplyr::select(Test, Run_Date, NOx_pe_flag, Mean_Chk_Conc) %>% rename(PE_Flag = NOx_pe_flag, PE_Conc = Mean_Chk_Conc) %>% mutate(PE_Target_Conc = NOx_pe)

PE_NH3_flag_disp <- NH3_peChk %>% dplyr::select(Test, Run_Date, NH3_pe_flag, Mean_Chk_Conc) %>% rename(PE_Flag = NH3_pe_flag, PE_Conc = Mean_Chk_Conc) %>% mutate(PE_Target_Conc = NH3_pe)

PE_Flag_disp <- rbind(PE_PO4_flag_disp, PE_NOx_flag_disp,PE_NH3_flag_disp)

PE_Flag_join <- PE_Flag_disp %>% dplyr::select(Test, Run_Date, PE_Flag)

knitr::kable(PE_Flag_disp)



```

 \newpage
 
## Check NOx Reduction Efficiency 
```{r Assess reduction Efficiency, echo=FALSE}


#check on the reduction efficiency for the NOx test: 
#Pull out test stds 
red_eff_1 <- df_all %>%
  filter(Sample_Name %in% c("Nitrate Standard", "Nitrite Standard"))

red_eff <- red_eff_1 %>% group_by(Run_Date) %>% mutate(Eff_Percent = (Conc / 0.5)*100, Eff_Flag = ifelse(Eff_Percent >= 90, 'YES', 'NO, rerun'))

#Plot these efficencies 
red_effs_plot <-  ggplot(data = red_eff, aes(x = Sample_Name, y = Conc, fill=Eff_Flag)) +
       geom_bar(stat = 'identity') + 
       facet_wrap(.~Run_Date, scale = "free_x") +
       scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkred")) +
       theme_classic() + labs(x= " ", y="NOx (mg/L)", title=" ") + 
       theme(legend.position="bottom") +  
       geom_hline(yintercept= 0.5, linetype="dashed", 
                color = "black", linewidth=1)  + 
       guides(fill=guide_legend(title="Reduction Efficiency >90%"))+
      scale_x_discrete(drop = TRUE)
print(red_effs_plot)

#create a flag 
Red_Eff_2 <- red_eff %>% group_by(Run_Date) %>% mutate(Mean_RedEff = mean(Eff_Percent)) %>% ungroup() %>% distinct()

Red_Eff_disp <- Red_Eff_2 %>% mutate(Red_Eff_Flag = ifelse(Mean_RedEff >= 95,
       "Mean NOx Reduction Efficiency >90% - PROCEED",
       "Mean NOx Reduction Efficiency <90% - REASSESS")) %>% dplyr::select(Test, Run_Date, Red_Eff_Flag)

Red_Eff_join <- Red_Eff_disp

knitr::kable(Red_Eff_disp)


```

\newpage

## Analyze the Check Standards
```{r Check Standards, echo=FALSE}

#Pull out check standards from raw file 

All_chks <- df_all %>% filter(
  str_detect(Sample_Name, "CCV") |
    str_detect(Sample_Name, "1mg/L ammonia") |
    str_detect(Sample_Name, "0.15 mg/L")
) %>% group_by(Test) %>% mutate(
  rep = row_number(),
  RSV = sd(Conc) / mean(Conc),
  RSV_Flag = ifelse(
    RSV >= chk_flag,
    "RSV TOO HIGH - REASSESS",
    "RSV WITHIN RANGE - PROCEED" #Calculate RSV and create flags
  )
)  

#Create Data frame for displaying RSV flags
check_flags_disp <- All_chks %>% mutate(RSV_Cutoff = chk_flag) %>% dplyr::select(Test, Run_Date, RSV_Flag, RSV, RSV_Cutoff) %>% distinct()

knitr::kable(check_flags_disp)


Chks_diff <- All_chks %>% mutate(Diff = ifelse(
  Test == "Vanadium NOx",
  100 * (Conc - NOx_CCV) / NOx_CCV,
  ifelse(
    Test == "Ammonia 2",
    100 * (Conc - NH3_CCV) / NH3_CCV,
    ifelse(Test == "o-PHOS 0.3", 100 * (Conc - PO4_CCV) / PO4_CCV, -999) #Calculate Check Percent Difference
  )
)) %>% mutate(Diff_Flag = ifelse(Diff <= chk_conc_flag, "YES", "NO, rerun")) #Flag data


#Create Data frame of CCV values for plot
CCV_df <- tibble(Test = c("Vanadium NOx", "Ammonia 2", "o-PHOS 0.3"), 
       Target_CCV = c(NOx_CCV, NH3_CCV, PO4_CCV))

Dates_df <- Chks_diff %>% dplyr::select(Test, Run_Date) %>% distinct()

CCV_plot <- left_join(CCV_df, Dates_df, by = join_by(Test))
                                        
#plot the check standards
chks_plot <-  ggplot(data = Chks_diff, aes(x = rep, y = Conc, fill=Diff_Flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "chartreuse4", "NO, rerun" = "darkred")) +
        labs(x= " ", y="Concentration (mg/L)", title="Check Standards") + 
        facet_wrap(Run_Date~Test, scales = "free") +
        scale_x_discrete(labels = NULL, breaks = NULL) +
        geom_hline(data = CCV_plot, aes(yintercept=Target_CCV),
              linetype="dashed", color = "black", linewidth=1) + 
        guides(fill=guide_legend(title="% Difference <10%")) +
        theme_classic() +
        theme(legend.position="bottom")

chks_plot

#Create Plot for displaying check flags
Chks_flag_disp <- Chks_diff %>% group_by(Test, Run_Date) %>% mutate(Percent = 100*length(which(Diff_Flag == "YES"))/length(Diff_Flag)) %>% mutate(CHK_Flag = ifelse(Percent >= chks_flag, ">60% of Checks Pass - PROCEED", "<60% of Checks Pass - REASSESs")) %>% ungroup() %>% dplyr::select(Test, Run_Date, CHK_Flag) %>% distinct()

knitr::kable(Chks_flag_disp)

Chks_Flag_join <- Chks_flag_disp


```

\newpage

## Analyze Blanks 
```{r Analyze Blanks, echo=FALSE}


#Pull out check standards from raw file 
blks_df <- df_all %>% filter(str_detect(Sample_Name, "CCB") | str_detect(Sample_Name, "Blank")) %>% group_by(Test, Run_Date) %>% mutate(rep = row_number())


#Pull out samples from df_all to calc quantile
samples <- df_all %>% 
  filter(str_detect(Sample_Name, c("TEMPEST"))) %>% 
  group_by(Test, Run_Date) %>% 
  mutate(Quant = quantile(Conc, prob=c(.25)) ) %>% 
  dplyr::select(Test, Run_Date, Quant) %>% 
  distinct() %>% 
  mutate(Quant = ifelse(Test == "Vanadium NOx", ifelse(Quant <= NOx_dl, 0.0125, Quant), Quant))  %>% #If NOx Quantile is below detection limit make blank cutoff half of detection limit
  ungroup()

blks_all <- left_join(blks_df, samples, by = join_by(Test, Run_Date))



blks_flagged <- blks_all %>% mutate(BLK_Flag = ifelse(Conc <= Quant, "YES", "NO, rerun")) %>% group_by(Test, Run_Date) %>% mutate(BLKs_Percent = 100*length(which(BLK_Flag == "YES"))/(length(BLK_Flag)) ) %>% mutate(BLK_Pct_Flag = ifelse(BLKs_Percent >= 60, ">60% of Blanks Pass - PROCEED", "<60% of Blanks Pass - REASSESS")) #Flag blanks 

#Create Data frame for displaying blank flags
blks_disp <- blks_flagged%>% group_by(Test, Run_Date) %>% mutate(Mean_Blk_Conc = mean(Conc)) %>% dplyr::select(Test, Run_Date, BLK_Pct_Flag, Mean_Blk_Conc, Quant) %>%  rename(Quantile_25 = Quant) %>% distinct()

knitr::kable(blks_disp)

Blks_Flag_join <- blks_disp %>% dplyr::select(Test, Run_Date, BLK_Pct_Flag)

#plotting the blanks compared to the lower 25% of conc (what the flag is)

blks_plot <-  ggplot(data = blks_flagged, aes(x = rep, y = Conc, fill=BLK_Flag)) +
       geom_bar(stat = 'identity') + 
       facet_wrap(Test ~ Run_Date, scales = "free") +
       scale_x_discrete(labels = NULL, breaks = NULL) +
       scale_fill_manual(values = c("YES" = "deepskyblue3", "NO, rerun" = "darkred")) +
       theme_classic() + labs(x= " ", y="Concentration (mg/L)", title="Blanks") + 
       theme(legend.position="bottom") +  
       geom_hline(aes(yintercept= Quant), linetype="dashed", 
                color = "black", linewidth=1)  + 
       guides(fill=guide_legend(title="Blank Conc <25% Quartile Samples"))

blks_plot

```

\newpage

## Analyze Duplicates
```{r Analyze Duplicates, echo=FALSE}

Duplicates <- df_all_cor %>% filter(Sample_Name == "Duplicate") %>%
  mutate(Sample_Num = Run_Number - 1) %>% #Get Run Number of Original Duplicated Sample
  mutate(Date_Num = paste0(Sample_Num, "_", Run_Date, "_", Test)) %>% #Create ID based on Run Date Test, and Run Number
  dplyr::select(Date_Num, Conc, Absorbance, Dilution) %>% #Select what we need
  rename(
  Dup_Conc = Conc,
  Dup_Absorbance = Absorbance,
  Dup_Dilution = Dilution
)

Dup_ready <- df_all_cor %>% mutate(Date_Num = paste0(Run_Number, "_", Run_Date, "_", Test)) #Create ID for original samples

Dup_Joined <- left_join(Duplicates, Dup_ready, by = join_by(Date_Num)) #Join originals and duplicates

Dup_plot_df <- Dup_Joined %>% filter(!Sample_Name %ilike% "chk") %>% group_by(Date_Num) %>% mutate(Duplicate_CV = cv(c(Dup_Conc, Conc))) %>% #Calculate cv of duplicates
  mutate(Dup_Flag = ifelse(Duplicate_CV >= 10, "NO, rerun", "YES")) %>% #Flags Dups
  ungroup() %>%
  group_by(Sample_Name) %>% ungroup() %>% group_by(Sample_Name) %>% 
  mutate(
  Num_Runs = length(Sample_Name),
  Max_Dil = max(Dilution),
  Rerun = ifelse(
    Num_Runs != 1 ,
    ifelse(Dilution == Max_Dil, "KEEP", "REMOVE"),
    "KEEP"
  ) ) %>%
  filter(Rerun == "KEEP") %>% mutate(Dilution = ifelse(Dilution == 0, 1, Dilution )) %>% mutate(Within_Range = case_when(
      Test == "Vanadium NOx" & Conc/Dilution < NOx_dl ~ "No, bdl",
      Test == "Vanadium NOx" & Conc/Dilution > NOx_top    ~ "No, adl",
      Test == "Vanadium NOx"               ~ "Within_Range",
      
      Test == "Ammonia 2" & Conc/Dilution < NH3_dl ~ "No, bdl",
      Test == "Ammonia 2" & Conc/Dilution > NH3_top    ~ "No, adl",
      Test == "Ammonia 2"                ~ "Within_Range",
      
      Test == "o-PHOS 0.3" & Conc/Dilution < PO4_dl  ~ "No, bdl",
      Test == "o-PHOS 0.3" & Conc/Dilution > PO4_top   ~ "No, adl",
      Test == "o-PHOS 0.3"                ~ "Within_Range",
      
      TRUE ~ NA_character_  # fallback for unexpected values
    ))#Only Keep Latest Dilution of Duplicates

#Plot Duplicates
Dup_Plot <-  ggplot(Dup_plot_df, aes(x = Sample_Name, y = Duplicate_CV, fill =  Dup_Flag, color = Within_Range)) +
  geom_bar(stat = 'identity',
    position = position_dodge2(preserve = "single"), linewidth = 0.75) +
  scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkred"), name = "CV below 10% ?") +
  geom_hline(yintercept = 10, linetype = "dashed" ) +
  facet_wrap(Test~Run_Date, scales = "free")  + 
  theme_classic() + 
    theme(legend.position = "bottom", strip.text = element_text(size = 8),
          axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 7)) +
   labs(title = "Duplicate Check")

Dup_Plot

#Create Data Frame for Flagging and Displaying Duplicates QAQC
Flags_Duplicates <- Dup_plot_df %>% group_by(Run_Date, Test) %>% mutate(Dup_Flags = ifelse(length(which(Dup_Flag == "YES"))/(length(Dup_Flag)) >= 0.6,  ">60% of Dups Pass - PROCEED", "<60% of Dups Pass - REASSESS")) %>% ungroup() %>% dplyr::select(Test, Run_Date, Dup_Flags) %>% distinct()

knitr::kable(Flags_Duplicates)

Dup_Flags_join <- Flags_Duplicates




```

\newpage

## Spikes
```{r Analyze Spikes, echo=FALSE}

Spikes <- df_all_cor %>% filter(Sample_Name == "Auto Spike") %>% mutate(Sample_Num = Run_Number - 1) %>% mutate(Date_Num = paste0(Sample_Num, "_", Run_Date, "_", Test)) %>% dplyr::select(Date_Num, Conc, Absorbance, Dilution) %>% rename(
  Spike_Conc = Conc,
  Spike_Absorbance = Absorbance,
  Spike_Dilution = Dilution
)

Spike_ready <- df_all_cor %>% mutate(Date_Num = paste0(Run_Number, "_", Run_Date, "_", Test))

Spike_Joined <- left_join(Spikes, Spike_ready, by = join_by(Date_Num))

Spike_Val <- tibble(
    Test = c("Ammonia 2", "o-PHOS 0.3", "Vanadium NOx"),
    Spike = c(NH3_Spk, PO4_Spk, NOx_Spk)
  )

Spike_Joined2 <- left_join(Spike_Joined, Spike_Val, by = join_by(Test))

Spike_plot_df <- Spike_Joined2 %>% filter(!Sample_Name %ilike% "ppt") %>% group_by(Date_Num) %>% mutate(Spike_Exp_Recovery = Conc + Spike) %>% 
  mutate(Spike_CV = cv(c(Spike_Exp_Recovery, Spike_Conc))) %>% 
  mutate(
    Spike_Flag = ifelse(
        Spike_CV >= 50,    #SEAL Uses 50% CV cutoff to check Spikes
      "No, rerun",
      "YES"
      )) %>% ungroup()

Spike_Plot <-  ggplot(Spike_plot_df, aes(x = Sample_Name, y = Spike_CV, fill =  Spike_Flag)) +
  geom_bar(stat = 'identity',
    position = position_dodge2(preserve = "single"), linewidth = 0.75) +
  scale_fill_manual(values = c("YES" = "darkgreen", "No, rerun" = "darkred"), name = "Spike CV <50% ?") +
  geom_hline(yintercept = c(50), linetype = "dashed" ) +
  facet_wrap(Test~Run_Date, scales = "free_x") + 
  theme_classic()  + 
    theme(legend.position = "bottom", strip.text = element_text(size = 8),
          axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 7)) +
   labs(title = "Spike Check")


Spike_Plot


Flags_Spike <- Spike_plot_df %>% group_by(Run_Date, Test) %>% mutate(Spike_Flags = ifelse(length(which(Spike_Flag == "YES"))/(length(Spike_Flag)) >= 0.6,  ">60% of Spikes have a CV <50% - PROCEED", ">60% of Spikes have a CV <50% - REASSESS")) %>% ungroup() %>% dplyr::select(Test, Run_Date, Spike_Flags) %>% distinct()

knitr::kable(Flags_Spike)

Spk_Flags_join <- Flags_Spike



```

\newpage

## Matrix Effects
```{r Analyze Matrix Effects, echo=FALSE}

Matrix <- df_all_cor %>% filter(Sample_Name == "Auto Spike") %>% mutate(Sample_Num = Run_Number - 1) %>% mutate(Date_Num = paste0(Sample_Num,"_", Run_Date, "_", Test)) %>% dplyr::select(Date_Num, Conc, Absorbance, Dilution) %>% rename(Spike_Conc = Conc, Spike_Absorbance = Absorbance, Spike_Dilution = Dilution)

Spike_ready <- df_all_cor %>% mutate(Date_Num = paste0(Run_Number, "_", Run_Date, "_", Test))

Spike_Joined <- left_join(Spikes, Spike_ready, by = join_by(Date_Num))

Spike_Val <- tibble(
    Test = c("Ammonia 2", "o-PHOS 0.3", "Vanadium NOx"),
    Spike = c(NH3_Spk, PO4_Spk, NOx_Spk)
  )

Spike_Joined2 <- left_join(Spike_Joined, Spike_Val, by = join_by(Test))

Spike_plot_df <- Spike_Joined2 %>% filter(Sample_Name %ilike% "ppt") %>% group_by(Date_Num) %>% mutate(Spike_Exp_Recovery = Conc + Spike) %>% 
  mutate(Spike_CV = cv(c(Spike_Exp_Recovery, Spike_Conc))) %>% 
  mutate(
    Spike_Flag = ifelse(
        Spike_CV >= 50,    #SEAL Uses 50% CV cutoff to check Spikes
      "No, rerun",
      "YES"
      )) %>% ungroup()

Spike_Plot <-  ggplot(Spike_plot_df, aes(x = Sample_Name, y = Spike_CV, fill =  Spike_Flag)) +
  geom_bar(stat = 'identity',
    position = position_dodge2(preserve = "single"), linewidth = 0.75) +
  scale_fill_manual(values = c("YES" = "darkgreen", "No, rerun" = "darkred"), name = "Matrix Spike CV <50% ?") +
       scale_x_discrete(name =NULL) +
  geom_hline(yintercept = c(50), linetype = "dashed" ) +
  facet_wrap(Test~Run_Date, scales = "free_x") + 
  theme_classic()  + 
    theme(legend.position = "bottom", strip.text = element_text(size = 8),
          axis.text.x = element_text(angle =30, hjust = 1, vjust = 1, size = 8)) +
   labs(title = "Matrix Check") +
  ylab("Matrix Spike CV")


Spike_Plot


Flags_Matrix <- Spike_plot_df %>% group_by(Run_Date, Test) %>% mutate(Matrix_Flags = ifelse(length(which(Spike_Flag == "YES"))/(length(Spike_Flag)) >= 0.6,  "Matrix Has CV <50% - PROCEED", "Matrix Has CV >50% - REASSESS")) %>% ungroup() %>% dplyr::select(Test, Run_Date, Matrix_Flags) %>% distinct()

knitr::kable(Flags_Matrix)

Matrix_Flags_join <- Flags_Matrix


```

```{r Add unit conversion, include=FALSE}

#Pull out samples 
samples <- df_all_cor %>%  
  filter(str_detect(Sample_Name, c("TEMPEST")))     


#Convert values based on the Test ID 
samples <- samples %>%
  mutate(
    Conc_uM = case_when(
      Test == "Vanadium NOx" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
      Test == "Ammonia 2" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
      Test == "o-PHOS 0.3" ~ (((as.numeric(samples$Conc))/Con1)/P_mw)*Con2,
      TRUE ~ as.numeric(NA)  # fallback in case of unexpected value
    ),
    # Replace negatives with 0 in 'value' and 'value_converted'
    Conc = pmax(Conc, 0),
    Conc_uM = pmax(Conc_uM, 0)
  )

head(samples)

```

##Sample Flagging - Within range of standard curve
```{r Sample Flagging, echo=FALSE}

#Flagging data if the concentration is outside the standards range 
samples_flagged1 <- samples %>% mutate(Dilution = ifelse(Dilution == 0, 1, Dilution )) %>%
  mutate(
    Conc_flag = case_when(
      Test == "Vanadium NOx" & Conc/Dilution < NOx_dl ~ "bdl",
      Test == "Vanadium NOx" & Conc/Dilution > NOx_top    ~ "adl",
      Test == "Vanadium NOx"               ~ "Within_Range",
      
      Test == "Ammonia 2" & Conc/Dilution < NH3_dl ~ "bdl",
      Test == "Ammonia 2" & Conc/Dilution > NH3_top    ~ "adl",
      Test == "Ammonia 2"                ~ "Within_Range",
      
      Test == "o-PHOS 0.3" & Conc/Dilution < PO4_dl  ~ "bdl",
      Test == "o-PHOS 0.3" & Conc/Dilution > PO4_top   ~ "adl",
      Test == "o-PHOS 0.3"                ~ "Within_Range",
      
      TRUE ~ NA_character_  # fallback for unexpected values
    )
  )

```

##Add QAQC Flags
```{r add QAQC Flags, echo = FALSE}
Flags_1 <- left_join(R2_Flags_join, PE_Flag_join, by = join_by(Test, Run_Date))

Flags_2 <- left_join(Flags_1, Red_Eff_join, by = join_by(Test, Run_Date) )

Flags_3 <- left_join(Flags_2, Chks_Flag_join, by = join_by(Test, Run_Date) )

Flags_4 <- left_join(Flags_3, Blks_Flag_join, by = join_by(Test, Run_Date) )

Flags_5 <- left_join(Flags_4, Spk_Flags_join, by = join_by(Test, Run_Date) )

Flags_6 <- left_join(Flags_5, Dup_Flags_join, by = join_by(Test, Run_Date) )
  
Flags_joined <- left_join(Flags_6, Matrix_Flags_join, by = join_by(Test, Run_Date) )

Flags_Bad <- Flags_joined %>% mutate(across(!1:2, \(x) ifelse(x %ilike% "PROCEED" | x %ilike% "PASS" | x %ilike% "NA" | is.na(x), "", x))) %>% distinct()

Flags_Fin <- Flags_Bad %>% mutate(QAQC_Flag = paste0(R2_Flag, PE_Flag, Red_Eff_Flag, CHK_Flag, BLK_Pct_Flag, Dup_Flags, Spike_Flags, Matrix_Flags)) %>% dplyr::select(Test, Run_Date, QAQC_Flag)
```

## Check to see if samples run match metadata & merge info
```{r check sample ids with metadata, echo=FALSE}

#check to see if all samples are present in the metadata 
all_present <- all(samples_flagged1$Sample_Name %in% nutr_metadata$NUTR_ID)

if (all_present) {
  message("All sample IDs are present in metadata.")
} else {
  message("Some sample IDs are missing from metadata.")
  
  # Optional: Which ones are missing?
  missing_ids <- setdiff(samples_flagged1$Sample_Name, nutr_metadata$NUTR_ID)
  print(missing_ids)
}
colnames(samples_flagged1)
samples_flagged_selected <- samples_flagged1 %>%
  dplyr::select(-c(Pair_ID)) 

#merge metadata with sample run data 
df_all_clean <- samples_flagged_selected %>% dplyr::select(Sample_Name, Test, Conc, Conc_uM, Conc_flag, Run_Date) %>%
  left_join(nutr_metadata, by = c("Sample_Name" = "NUTR_ID")) %>% left_join(Flags_Fin, by = join_by(Test, Run_Date))

```



##Format Data
```{r Export Processed Data, include=FALSE}

#pivot the data set wider to make it wide format 
final_data1 <- df_all_clean %>%
  pivot_wider(
    id_cols = Sample_Name,
    names_from = Test,
    values_from = Conc,
    names_glue = "{Test}_Conc"
  )

final_data2 <- df_all_clean %>%
  pivot_wider(
    id_cols = Sample_Name,
    names_from = Test,
    values_from = Conc_uM,
    names_glue = "{Test}_Conc_uM"
  )

final_data3 <- df_all_clean %>%
  pivot_wider(
    id_cols = Sample_Name,
    names_from = Test,
    values_from = Conc_flag,
    names_glue = "{Test}_Conc_flag"
  )

final_data4 <- df_all_clean %>%
  pivot_wider(
    id_cols = Sample_Name,
    names_from = Test,
    values_from = QAQC_Flag,
    names_glue = "{Test}_QAQC_Flag"
  )

#Select only one Sample ID
df_all_clean_cols_one_row <- df_all_clean %>%
  group_by(Sample_Name) %>%
  slice(1) %>%  # or use summarise() if you want to aggregate
  ungroup()

#merge these together
data_list <- list(df_all_clean_cols_one_row, final_data1, final_data2, final_data3, final_data4)

final_data4 <- reduce(data_list, full_join, by = c("Sample_Name"))


#Add project information 
final_data_labeled <- final_data4 %>% 
  mutate(
    Site = "TEMPEST",   # new column with same value on every row
    Run_notes = run_notes, 
    Analysis_rundate = Run_Date# new column with notes about the run
  ) 

#Prepare data to be exported 
final_data <- final_data_labeled %>%
  mutate(
    Depth_cm = gsub("cm", "", Depth)
  ) %>%
    rename(
    Sample_ID = Sample_Name, 
  
    NOx_Conc_mgL = `Vanadium NOx_Conc`,
    NOx_Conc_uM = `Vanadium NOx_Conc_uM`,
    NOx_Conc_Flag = `Vanadium NOx_Conc_flag`,
    NOx_QAQC_Flag = `Vanadium NOx_QAQC_Flag`,
    
    NH3_Conc_mgL = `Ammonia 2_Conc`,
    NH3_Conc_uM = `Ammonia 2_Conc_uM`,
    NH3_Conc_Flag =`Ammonia 2_Conc_flag`,
    NH3_QAQC_Flag = `Ammonia 2_QAQC_Flag`,
    
    PO4_Conc_mgL = `o-PHOS 0.3_Conc`,
    PO4_Conc_uM = `o-PHOS 0.3_Conc_uM`,
    PO4_Conc_Flag =`o-PHOS 0.3_Conc_flag`,
    PO4_QAQC_Flag = `o-PHOS 0.3_QAQC_Flag`,
    
    Field_notes = Notes, 
    Evacuation_Date = Evacuation_date_YYYMMDD
    # add more rename pairs as needed
  ) %>%
  dplyr::select(Sample_ID, Site, Zone, Grid, Depth_cm, Event_Time, Time_of_day, 
         Source, Volume_mL, Collection_Date, Evacuation_Date, 
          NOx_Conc_mgL, NOx_Conc_uM, NOx_Conc_Flag, NOx_QAQC_Flag,
         NH3_Conc_mgL, NH3_Conc_uM, NH3_Conc_Flag, NH3_QAQC_Flag,
         PO4_Conc_mgL, PO4_Conc_uM, PO4_Conc_Flag, PO4_QAQC_Flag,
         Analysis_rundate,  Run_notes, Field_notes)

```

## Visualize Data by Plot   
```{r Visualize Data, echo=FALSE, warning=FALSE}

#Plot samples to get a first look at concentrations (sanity check)
data_plotting <- final_data

#Order by Zone from Upland to Surface Water
data_plotting$Zone = factor(data_plotting$Zone, levels = c("C", "FW", "SW"))
data_plotting <- data_plotting[order(data_plotting$Zone), ]

#group the data for plotting
data_plotting <- data_plotting %>%
  mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
  ungroup()

#Plot data and change colors based on Zone:
viz_NH3_plot <- ggplot(data_plotting, aes(x = row_num, y = NH3_Conc_uM, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_wrap(~ Zone, scales="free") +
  scale_fill_manual(values = c(
    "C" = "springgreen2",
    "FW" = "cyan2",
    "SW" = "violetred2"
  )) +
  theme_classic() +
  theme(axis.text.x = element_blank()) +
  labs(x = " ", y = "NH3 (uM)", title = "Samples: Ammonia") +
  scale_x_discrete(drop = TRUE)


viz_PO4_plot <-  ggplot(data_plotting, aes(x = row_num, y = PO4_Conc_uM, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_wrap(~ Zone, scales="free") +
  scale_fill_manual(values = c(
    "C" = "springgreen2",
    "FW" = "cyan2",
    "SW" = "violetred2"
  )) +
  theme_classic() +
  theme(axis.text.x = element_blank()) +
  labs(x = " ", y = "PO4 (uM)", title = "Samples: Phosphate") +
  scale_x_discrete(drop = TRUE)

viz_NOx_plot <-  ggplot(data_plotting, aes(x = row_num, y = NOx_Conc_uM, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_wrap(~ Zone, scales="free") +
  scale_fill_manual(values = c(
    "C" = "springgreen2",
    "FW" = "cyan2",
    "SW" = "violetred2"
  )) +
  theme_classic() +
  theme(axis.text.x = element_blank()) +
  labs(x = " ", y = "NOx (uM)", title = "Samples: Nitrate/Nitrite") +
  scale_x_discrete(drop = TRUE)

ggarrange(viz_NH3_plot, viz_PO4_plot, viz_NOx_plot, nrow=3, ncol=1)


```

##Write Out Data
```{r Export Data, echo = FALSE}

#Write out data frame 
write.csv(final_data, final_path)
  

```




#end



