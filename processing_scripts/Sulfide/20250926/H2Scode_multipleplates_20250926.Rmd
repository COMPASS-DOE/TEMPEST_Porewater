---
title: "TEMPEST: Porewater Sulfide"
author: "2025 Samples"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: false
    number_sections: false
geometry: "left=2cm,right=2cm,top=1cm,bottom=2cm"
output_dir: "To Be Reviewed/PDF"
---

```{r Add Required Packages, include=FALSE}

library(dplyr)
library(broom)
library(ggplot2)
library(ggpubr)
library(stringr)
library(purrr)
library(tidyverse)
library(here)
library(data.table)
library(matrixStats)
library(gridExtra)
library(grid)
library(plater)
library(raster)
library(knitr)  
library(readxl)
library(patchwork)

#Packages for loading metadata
require(pacman)
pacman::p_load(janitor, # useful for simplifying column names
               googlesheets4, # read_sheet 
               googledrive, # drive_ functions
               plotrix,
               here) 

```

## Run Information
```{r Run Information, echo=TRUE, message=FALSE, warning=FALSE}

###things that need to be changed
Date_Run = "20250926"
plates<- c("Plate8","Plate9","Plate10","Plate11","Plate12","Plate13","Plate14","Plate15")
Month = "Sept"
Year = "2025"
Run_by = "Zoe Read"  #Instrument user 
Script_run_by = "Zoe Read" #Code user 
Project = "COMPASS"

run_notes="MC 10 was low in 4/8 plates. Plate 1 and 4 dups were bad. 
2/8 dups were bad. Used 10 uL spike for all plates. 
3/8 spikes were bad. Plate 15 Std curve used for all plates."#any notes from run

#Stds that should be excluded
# stds_to_remove<-data.frame(Plate=c("Plate6"),IDs=c("Std 3"))
stds_to_remove<-NA

```


```{r File Names, echo=TRUE, message=FALSE, warning=FALSE, include = FALSE}

###### File Names - - PLEASE CHECK, should automatically update based on Date and Plate Number

#read in all the Tidy data files
file_paths_tidydata <- list.files(path = "Tidy Data", full.names = TRUE)
head(file_paths_tidydata)

#file path name for processed data
#Write out final data frame 
final_data_path <- "TEMPEST_H2S_20250926_Plates_Combined.csv"
HighCV_path <- "TEMPEST_H2S_20250926_Plates_HighCVs.csv"
ADL_not_fixed_path <- "TEMPEST_H2S_20250926_Plates_ADL.csv"
BDL_not_fixed_path <- "TEMPEST_H2S_20250926_Plates_BDL.csv"

#QAQC log path
log_path="Raw Data/Sulfide_STD_QAQC.csv"

#Sample ID Path
SampleIDs1=paste0("Raw Data/TEMPEST_2025_SampleIDs_20250926.xlsx")

#Metadata path
Raw_Metadata = "Raw Data/Sulfide_STD_QAQC.csv"

```

```{r Set Up Code - constants and QAQC cutoffs, echo=TRUE, message=FALSE, warning=FALSE, include = FALSE}

#Flag cutoffs
  r2_cutoff = 0.98            #this is the level below which we want to rerun or consider a curve 
  cv_flag_stds = 10           #this is the maximum cv allowed for standards
  p_value_chkstds = 0.05      #the p-value for the t-test between check standards must be greater than this
  p_value_MC = 0.05           #the p-value for the t-test between matrix checks and top std must be greater than this
  cv_flag_sample = 10         #this is the maximum cv allowed for samples
  dups_perc_diff = 15.5       #this is the maximum percent difference allowed between duplicates
  high_recovery_cutoff = 120  #this is the maximum percent recovery of SO4 allowed in spiked samples
  low_recovery_cutoff = 80    #this is the minimum percent recovery of SO4 allowed in spiked samples

#Standard concentrations - Update if running different standard curve:
# standard units are in uM
  standards <- tibble(
    IDs = c("Std 0", "Std 1", "Std 2", "Std 3", "Std 4", "Std 5"),
    Conc = c(0, 5.0, 12.5, 25.0, 50.0, 100.0))  #uM

#Spike concentration calc
  #spike for these samples was 50 uL of the 100uM standard
  Con1 <- 1000000               #conversion factor value for spike volumes (uL -> L)
  spk_std <- 100                # uM S2- standard used
  spkvol <- 10                  # uL volume of spike added
  spk_Conc <- spk_std*(spkvol/Con1)  # umoles of S2- added to each spiked sample
  sample_vol <- 250             # the sample volume without the spike is 250 uL

#Top standard Concentration- Update if running different standard curve: 
   Top_STD = 100
   
#Set time zone 
  common_tz = "Etc/GMT+5"
  Sys.setenv(TZ = "America/New_York")

```


```{r Read in tidy data, echo=TRUE, message=FALSE, warning=FALSE, include = FALSE, results = 'hide'}

#read in the csv files
file_paths_tidydata

dat<-read_plates(
  files = file_paths_tidydata,            #list of all file paths
  plate_names = file_paths_tidydata ,     #list of plate names          
  well_ids_column = "Wells",              # name to give column of well IDs (optional)
  sep = ","                               # separator used in the csv file (optional)
) %>%
  rename("Abs"=values,
         "IDs"=values.2,
         "Dilution"=values.3)

dat <- dat %>%
  mutate(Analysis_rundate = str_extract(Plate, "\\d{8}"))%>%
  mutate(Plate = str_extract(Plate, "Plate[0-9]+"))
  

head(dat)
  
```

```{r Fix data entered incorrectly, echo=TRUE, message=FALSE, warning=FALSE, include = FALSE, results = 'hide'}
##Data entered incorrectly
# dat$IDs[dat$IDs == Old_ID_1] <- New_ID_1
# dat$IDs[dat$IDs == Old_ID_2] <- New_ID_2
# dat$IDs[dat$IDs == Old_ID_3] <- New_ID_3
# dat$IDs[dat$IDs == Old_ID_4] <- New_ID_4
# dat$IDs[dat$IDs == Old_ID_5] <- New_ID_5
# dat$IDs[dat$IDs == Old_ID_6] <- New_ID_6

```


```{r First look at Stds,echo=FALSE,fig.keep='none',include=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
#subset by H2s Stds
H2S_stds_all <- dat %>% subset(IDs %like% "Std"|IDs %like% "MC") 
head(H2S_stds_all)

H2S_stds_all <- left_join(H2S_stds_all, standards, by = "IDs")

##Plot stds and calculate the slope, intercept, and R2
H2S_stds <- na.omit(H2S_stds_all)
H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)+
  ggtitle("Sulfide Standard Curve")

H2S
```


```{r Remove high cv stds,echo=FALSE,fig.keep='none',include=FALSE, results = 'hide'}

H2S_stds_all_fixed<-H2S_stds_all
#make a data frame for the averages 
H2S_stds1_all <- -data.frame(matrix(ncol = 8, nrow =0 ))
colnames(H2S_stds1_all)<-c("IDs", "H2S_mean_Abs", "H2S_sd", "H2S_cv", "Plate","Dilution",  "Conc", "H2S_cv_flag")
#make dataframe for fixed data
H2S_stds_HCV_fixed<-data.frame(matrix(ncol = 8, nrow =0 ))
colnames(H2S_stds_HCV_fixed)<-c("IDs", "H2S_mean_Abs", "H2S_sd", "H2S_cv", "Plate","Dilution",  "Conc", "H2S_cv_flag")


#make a loop for each plate 
for(y in 1:length(plates)){
# make a new dataframe for the high cv points for each plate
  H2S_stds_HCV_fixed<-data.frame(matrix(ncol = 8, nrow =0 ))
  colnames(H2S_stds_HCV_fixed)<-c("IDs", "H2S_mean_Abs", "H2S_sd", "H2S_cv", "Plate","Dilution",  "Conc", "H2S_cv_flag")
  
  #filter each plate    
  std_plate<- H2S_stds_all %>% subset(Plate == plates[y])
  
  #Calculate cv for STDs
  std_plate1 <- std_plate %>%
    group_by(Plate, Dilution, IDs) %>%
    summarise(H2S_mean_Abs = mean(Abs), H2S_sd = sd(Abs), H2S_cv = cv(Abs),
              Conc = first(Conc))
  #Flag high cvs
  std_plate1$H2S_cv_flag <- ifelse(std_plate1$H2S_cv > cv_flag_stds, 'High CV', 'Within range')
  
  #add each plate to std average dataframe
  H2S_stds1_all<-rbind(H2S_stds1_all,std_plate1)
  # filter High CV Samples
  H2S_stds1_all_HCV <- std_plate1 %>%  
    filter(str_detect(H2S_cv_flag, "High CV"))
  #this will keep the loop from breaking if there are no high cv points
  if(!nrow(H2S_stds1_all_HCV)==0){  
    
    #make a loop to filter each high cv sample
    for(x in 1:nrow(H2S_stds1_all_HCV)){
      sing_ID<- std_plate %>% subset(IDs == H2S_stds1_all_HCV$IDs[x])
      #make a dataframe for cvs
      cv_trial<-data.frame(matrix(ncol = 3, nrow =1 ))
      
      #make a loop to find the cv for each deleted point
      for(i in 1:nrow(sing_ID)){
        sing_ID1<-sing_ID[-i,] 
        cv_trial[i]<-cv(sing_ID1$Abs)
      }
      
      # delete the point that gives the lowest cv
      sing_ID<-sing_ID[-which.min(cv_trial),]
        H2S_stds_HCV_fixed<-rbind(H2S_stds_HCV_fixed,sing_ID)
    }
  #recalculate the average and cv  
  H2S_stds1_all_fixed <- H2S_stds_HCV_fixed %>%
      group_by(Plate, Dilution, IDs) %>%
      summarise(H2S_mean_Abs = mean(Abs), H2S_sd = sd(Abs), H2S_cv = cv(Abs),
              Conc = first(Conc))
  #Flag high cvs
  H2S_stds1_all_fixed$H2S_cv_flag <- ifelse(H2S_stds1_all_fixed$H2S_cv > cv_flag_stds, 'High CV', 'Within range')
  #bind fixed dataframes back together with non high cv points
  #averaged stds
  H2S_stds1_all <- subset(H2S_stds1_all, (!(IDs %in% H2S_stds1_all_fixed$IDs & Plate %in% H2S_stds1_all_fixed$Plate)))
  H2S_stds1_all <- rbind(H2S_stds1_all_fixed, H2S_stds1_all) 
  #all points  
  H2S_stds_all_fixed <- subset(H2S_stds_all_fixed, (!(IDs %in% H2S_stds_HCV_fixed$IDs & Plate %in% H2S_stds_HCV_fixed$Plate)))
  H2S_stds_all_fixed <- rbind(H2S_stds_HCV_fixed, H2S_stds_all_fixed) 
 }}

# filter High CV Stds
stds_HCV <- H2S_stds1_all %>%  
  filter(str_detect(H2S_cv_flag, "High CV")) 
head(stds_HCV)
```


```{r Plot all Standard Curves together, echo=FALSE,message=FALSE,fig.keep='none', results = 'hide'}
##this is to see if there are any stds or plates that look off
#Remove check standards which have NAs in the Conc column
H2S_stds <- na.omit(H2S_stds_all_fixed)
H2S_stds1 <- na.omit(H2S_stds1_all)
head(H2S_stds1)

##Plot stds from all plates together
H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)+
  ggtitle("Sulfide Standard Curve")
H2S
```


```{r R2 of all Standard Curves together, echo=FALSE,message=FALSE,fig.keep='none',include=FALSE, results = 'hide'}
H2S_lm <- lm(H2S_stds$Abs ~ H2S_stds$Conc)
summary(H2S_lm)
cf <- coef(H2S_lm)

#create data frame with 1 rows and 0 columns
Slopes <- data.frame(matrix(ncol = 0, nrow = 1))
Slopes$Curve <- "H2S"
Slopes$R2 <- summary(H2S_lm)$adj.r.squared
Slopes$Slope <- cf[2]
Slopes$Intercept <- cf[1]
print(Slopes)

```

```{r Plot the Std Curve for each plate, echo=FALSE, message=FALSE, results = 'hide', warning=FALSE, fig.keep='last'}
#make a qaqc flag column
dat$QAQC_flag<-"" 

#delete stds that are off for each plate
# for(i in 1:nrow(stds_to_remove)){
# H2S_stds<-H2S_stds %>% subset(!(Plate %in% stds_to_remove$Plate[i] & IDs%in%stds_to_remove$IDs[i]))}

#create a graph for each plate's std curves
curves<-function(plate_names){
  H2S_stds<- H2S_stds %>% filter(grepl(plate_names,H2S_stds$Plate ))
#Plot stds and calculate the slope, intercept, and R2 
H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)+ labs(y="Concentration (uM)", x="Absorbance",title = paste( plate_names,"STD Curve"))
}

#apply the function
lapply(plates,curves)

H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)+ 
  labs(y="Concentration (uM)", x="Absorbance",title = paste("STD Curves"))+facet_wrap(~Plate)+ theme_classic()+ 
  stat_regline_equation(label.x = 15, label.y = 1) + # Add the equation
  stat_regline_equation(aes(label = ..adj.rr.label..), label.x = 15, label.y = 0.8) # Add R squared
H2S

#make dataframe for the slope and intercept
qplate <- data.frame(matrix(ncol = 7, nrow = 0))
#determine the r2 slope and intercept for each plate
for(i in 1:length(plates)){
H2S_stds2<-H2S_stds %>% subset(Plate==plates[i])
H2S_lm <- lm(H2S_stds2$Abs ~ H2S_stds2$Conc)
summary(H2S_lm)
cf <- coef(H2S_lm)

#create data frame 
Date <- Date_Run
R2 <-summary(H2S_lm)$adj.r.squared
Slope <- cf[2]
Intercept <- cf[1]
Plate<- plates[i]
qplate1 <- data.frame(Date, Project , R2, Slope, Intercept, Top_STD, Plate, row.names = NULL)
qplate<-rbind(qplate, qplate1)

}
head(qplate)

#Was the standard curve ok for each plate? 
for(i in 1:nrow(qplate)){
  print(plates[i]) 
  if (qplate$R2[i] < (r2_cutoff)) {
    print("Std Curve r2 is below cutoff! - REASSESS")
  } else {
    print("Std Curve r2 GOOD")
  }
  #writes a flag for plates that have a low R2   
  dat$QAQC_flag<-if(qplate$R2[i] <= r2_cutoff){
    ifelse(dat$Plate %in% qplate$Plate[i], ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, "; Std curve r2 low"),"Std curve r2 low"),paste0( dat$QAQC_flag,"") )                   
  }else{
    ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, ""),"")
  } 
}

#Look a the percent cv for the std curves slopes and intercepts 
std_curve_cv <- data.frame(IDs=c("Slope","Intercept") ,mean = c(mean(qplate$Slope),mean(qplate$Intercept)) , H2S_sd = c(sd(qplate$Slope),sd(qplate$Intercept)),H2S_cv = c(cv(qplate$Slope),cv(qplate$Intercept)))

head(std_curve_cv)

###Should I print out flag for high cv?
```

## Checking STD Data against QAQC file
```{r Checking STD Data against QAQC file,message=FALSE, warning = FALSE, echo=FALSE,fig.keep='none', results = 'hide'}

#read in datafile with all the slopes
qlogO <- read.csv(log_path)
qlogO$X <- NULL
qlogO$Date<- as.character(qlogO$Date)
qlogO$"Plate" <- "All"
#head(qlogO)

#combine new slopes with log file
qplate<- anti_join(qplate,qlogO ,by = c("Date", "Project", "R2", "Slope", "Intercept"))
qlog<-rbind(qplate,qlogO)
#head(qlog)

##plot the slopes to make sure there are no crazy outliers 
slope1 <- ggplot(data=qlog, aes(x=Date, y=Slope)) +
            geom_hline(yintercept= (mean(qlog$Slope)+ (2*sd(qlog$Slope))), linetype="dashed", color = "red", linewidth=2)+
            geom_hline(yintercept= (mean(qlog$Slope)- (2*sd(qlog$Slope))), linetype="dashed", color = "red", linewidth=2)+
            geom_point() + 
            theme_classic() + ylim(0, 0.02) + 
            theme(legend.position="none") + 
            ggtitle("Sulfide Slopes")+
            guides(x = guide_axis(angle = 70)) +
            geom_point(data = qlog %>% filter(Date %in% qplate$Date & Slope%in%qplate$Slope),color = "orange")
slope1


#Rerun if outside of red lines

##plot the intercepts to make sure there are no crazy outliers 
int1 <- ggplot(data=qlog, aes(x=Date, y=Intercept)) +
          geom_hline(yintercept= (mean(qlog$Intercept)+ (2*sd(qlog$Intercept))), linetype="dashed", color = "red", linewidth=2)+
          geom_hline(yintercept= (mean(qlog$Intercept)- (2*sd(qlog$Intercept))), linetype="dashed", color = "red", linewidth=2)+
          geom_point() + 
          theme_classic() + ylim(0.05,0.125) + 
          theme(legend.position="none")+ 
          ggtitle("Sulfide Intercepts")+
          guides(x = guide_axis(angle = 70)) +
          geom_point(data = qlog %>% filter(Date %in% qplate$Date & Intercept%in%qplate$Intercept),color = "orange")
int1

#plot the R2s to make sure there are no crazy outliers 
Rsq1 <- ggplot(data=qlog, aes(x=Date, y=R2)) +
          geom_hline(yintercept= (0.98), linetype="dashed", color = "red", linewidth=2)+
          geom_point() + 
          theme_classic() + ylim(0.96, 1.01) + 
          theme(legend.position="none")+ 
          ggtitle("Sulfide R2s")+
          guides(x = guide_axis(angle = 70))+
          geom_point(data = qlog %>% filter(Date %in% qplate$Date & R2 %in% qplate$R2),color = "orange")
Rsq1

#Is the slope and intercept for each plate within 2 standard deviation of the log file?
for(i in 1:nrow(qplate)){
  print(plates[i])  
  
  if (qplate$Slope[i] > (mean(qlog$Slope)+ (2*sd(qlog$Slope)))| qplate$Slope[i] < (mean(qlog$Slope)- (2*sd(qlog$Slope)))) {
    print("Std curve slope is 2 sd different from previous slopes! - REASSESS")
  } else {
    print("Std curve slope is with 2 sd of previous slopes")
  }

  if (qplate$Intercept[i] > (mean(qlog$Intercept)+ (2*sd(qlog$Intercept)))| qplate$Intercept[i] < (mean(qlog$Intercept)- (2*sd(qlog$Intercept)))) {
    print("Std curve intercept is 2 sd different from previous intercepts! - REASSESS")
  } else {
    print("Std curve intercept is with 2 sd of previous intercepts")
  }
}



```


```{r Plot standard graphs and pick the best std curve, echo = FALSE, warning = FALSE, message = FALSE, fig.width=8, fig.height=12}

##Plot std graphs together
ggarrange(slope1,int1,Rsq1, nrow=3, ncol=1)

#pick the best std curve
std_curve<-qplate %>% filter(Intercept<(mean(qlog$Intercept)+ (2*sd(qlog$Intercept)))&Intercept>(mean(qlog$Intercept)- (2*sd(qlog$Intercept))))%>% filter(Slope<(mean(qlog$Slope)+ (2*sd(qlog$Slope)))&Slope>(mean(qlog$Slope)- (2*sd(qlog$Slope)))) %>% filter(R2 == max(R2))

knitr::kable(std_curve, caption = "Best std curve to use: ")

#stop knit if no Std curve satisfies the conditions
if (nrow(std_curve)==0) {
  stop("Error: No Std Curve is good -Reassess. Exiting early.")
}
```

## Matrix Check QAQC
```{r Matrix Checks,echo=FALSE,warning=FALSE, message=FALSE, results = 'hide'}

#filter out matrix stds
H2S_MC <- H2S_stds_all %>%  
  filter(str_detect(IDs, "MC")) 

H2S_MC_HCV <- H2S_stds1_all%>%
  filter(grepl("MC", IDs)) %>%  
  filter(str_detect(H2S_cv_flag, "High CV"))
head(H2S_MC_HCV)

#make a dataframe for matrix checks
QAMC<-data.frame(matrix(nrow=0,ncol=3))
colnames(QAMC) <- c("Plate","IDs","p_value")
        
#compare matrix checks to chosen std curve
for(i in 1:length(plates)){
  H2S_MC1<-filter(H2S_MC, Plate %in% plates[i])
  print(plates[i])
  
  ##Select Std 5 for comparison to MCs
  std5 <- subset(H2S_stds, IDs == "Std 5"& Plate ==std_curve$Plate)
  MC10 <- subset(H2S_MC1, IDs == "MC: 10ppt S5")
  MC20 <- subset(H2S_MC1, IDs == "MC: 20ppt S5") 

  #10ppt matrix check
  if("MC: 10ppt S5" %in% MC10$IDs){
    t.test.MC10 <- t.test(std5$Abs,MC10$Abs,var.equal = T)
    t.test.MC10

    if(t.test.MC10$p.value > p_value_MC){
      print("Matrix Check 10 GOOD")
    } else {
      print("Matrix Check 10 is signficantly different from Std 5 - REASSESS")
    }
  #write out a flag to the sample dataframe if the MC is Bad
  dat$QAQC_flag<-if(t.test.MC10$p.value < p_value_chkstds){
    ifelse(dat$Plate %in% qplate$Plate[i], ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, "; MC 10 out of range"),"MC 10 out of range"),paste0( dat$QAQC_flag,"") )                   
  }else{
    ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, ""),"")
  } 
  #Add p-value to dataframe
  QAMC<-rbind(QAMC,list(Plate=plates[i],IDs="MC: 10ppt S5
",p_value=t.test.MC10$p.value))
  }else{
    #if std didn't have enough points for t-test 
    print("Did not run 10ppt matrix")
  }

  #20ppt matrix check
  if("MC: 20ppt S5" %in% MC20$IDs){
    t.test.MC20 <- t.test(std5$Abs,MC20$Abs,var.equal = T)
    t.test.MC20
 
    if(t.test.MC20$p.value > p_value_MC){
      print("Matrix Check 20 GOOD")
    } else { 
      print("Matrix Check 20 is signficantly different from Std 5 - REASSESS")
    }
#write out a flag to the sample dataframe if the MC is Bad
  dat$QAQC_flag<-if(t.test.MC20$p.value < p_value_chkstds){
    ifelse(dat$Plate %in% qplate$Plate[i], ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, "; MC 20 out of range"),"MC 20 out of range"),paste0( dat$QAQC_flag,"") )                   
  }else{
      ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, ""),"")
  } 
  #Add p-value to dataframe
  QAMC<-rbind(QAMC,list(Plate=plates[i],IDs="MC: 20ppt S5
",p_value=t.test.MC20$p.value))

  }else{
    #if std didn't have enough points for t-test 
    print("Did not run 20ppt matrix")
  }
}

##Matrix Checks Plot
##plot MC Abs vs Std 5 Abs and color based on whether passed T test
##Should I plot all the absorbances or just the average? 

#Flag p-values
QAMC$p_flag <-  ifelse(QAMC$p_value > p_value_MC, 'OK', 'Rerun')

##Add the p-values to abs values
std5_avg <- mean(std5$Abs)
H2S_MC_avg <- H2S_MC %>%  
  group_by(Plate, IDs) %>%
  summarise(mean_abs = mean(Abs, na.rm = TRUE),) %>%
  ungroup()

QAMC$IDs <- gsub("\\n", "", QAMC$IDs)

QAMC_abs <- merge(QAMC, H2S_MC_avg, by = c("Plate", "IDs"))

QAMC_abs$p_flag

# head(QAdups)

#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
MCbar <- ggplot(data = QAMC_abs, aes(x = IDs, y = mean_abs, fill=p_flag)) +
        geom_bar(stat = 'identity') + facet_wrap(~Plate)+
        scale_fill_manual(values = c("OK" = "darkgreen", "Rerun" = "darkred"))+
        theme_classic() + labs(x= "Sample ID", y="MC absorbance values compared to std 5") + 
        theme(legend.position="none") +  
        geom_hline(yintercept=std5_avg, linetype="dashed", color = "black", linewidth=1) + 
        annotate("label", x = 1.5, y = (std5_avg), label = "Std 5 Abs", color = "black", fontface = "bold", fill = "white") +
        ggtitle("Matrix Effects")

MCbar

```


```{r Calculate Sulfide Concentrations,echo=FALSE,include=FALSE, results = 'hide', message=FALSE}

#filter out stds
dat <- dat %>%  
      filter(!str_detect(IDs, "Std")) %>%  
      filter(!str_detect(IDs, "MC"))

#Calculate concentrations of Sulfide
dat$Conc <- ((dat$Abs-std_curve$Intercept)/std_curve$Slope)*(dat$Dilution)

#Use ifelse to make any negative values equal to zero
dat$H2S_Conc_Final <- ifelse(dat$Conc <0, 0, dat$Conc)

#mark samples that are below detection limit and change to zero
dat$H2S_info <- ifelse(dat$Conc <=0, 'bdl', ifelse(dat$Abs > mean(std5$Abs), "adl", "Within Range")) 

head(dat)
```

  
```{r Calculate Averages across wells and sd,echo=FALSE, fig.keep='last', message=FALSE, warning=FALSE, fig.width=8, fig.height=12, results = 'hide'}

#summarize by Plate, Dilution, and sampleID so that we can calculate the mean and std. dev. of the three wells 
dat1 <- dat %>%
  group_by(Plate, Dilution, IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final),
            QAQC_flag = first(QAQC_flag), Analysis_rundate = first(Analysis_rundate))

#Flag high cvs 
dat1$H2S_cv_flag <- ifelse(dat1$H2S_cv > cv_flag_sample, 'High CV', 'Within Range')

# to flag high dilutions 
dat1$H2S_flag <- ifelse(dat1$H2S_mean <=0, 'bdl', ifelse(dat1$H2S_mean > ((mean(std5$Abs)-std_curve$Intercept)/std_curve$Slope)*(dat1$Dilution), "adl", "Within Range"))

head(dat1)

#plot data and sd's just to check and see what they look like - just a quick first look
H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean, color=H2S_flag))+
  geom_point(size=4)+
  scale_color_manual(values = c("abl" = "red", "bdl" = "orange", "Within Range" = "black")) +  theme_classic() +
  labs(y="Sulfide (uM)", x="Sample ID",title=paste(Project, Month, Year," Sulfide Data")) +
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
  guides(x = guide_axis(angle = 70))+ 
  facet_wrap(~Plate, scales = "free")
H2S

#quick look at high cvs
H2S_original <- ggplot(dat1, aes(x=IDs, y=H2S_mean, color = H2S_cv_flag))+ 
  geom_point(size=4) +  theme_classic() + 
  scale_color_manual(values = c("Within Range" = "darkgreen", "High CV" = "darkred"))+
  labs(y="Sulfide (uM)", x="Sample ID") + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
  guides(x = guide_axis(angle = 70)) + 
  facet_wrap(~Plate, scales = "free", ncol = 2) + 
  ggtitle("Sample triplicate means and sd dev before bad reps removed") 
H2S_original

```


## Remove bad reps
```{r remove bad reps,echo=FALSE, results = 'hide', fig.width=8, fig.height=12}

##Need to make it so it groups by Plate and Dilution so that reruns of the same ID don't count towards the CV
dat1$ID_full <- paste(dat1$Plate, dat1$Dilution, dat1$IDs, sep = "_")
dat$ID_full <- paste(dat$Plate, dat$Dilution, dat$IDs, sep = "_")

##auto remove bad reps
# filter High CV Samples
dat1_HCV <- dat1 %>%  
  filter(str_detect(H2S_cv_flag, "High CV")) 
dat_HCV <- subset(dat, (ID_full %in% dat1_HCV$ID_full ))
#make dataframe for fixed data
dat_HCV_fixed<-data.frame(matrix(ncol = 10, nrow =0 ))
colnames(dat_HCV_fixed)<-c("Wells", "Abs", "IDs", "Dilution", "QAQC_flag", "Conc", "H2S_Conc_Final", "H2S_info", "Plate", "ID_full")



#make a loop to filter each high cv sample
for(x in 1:nrow(dat1_HCV)){ 
  sing_ID<- dat_HCV %>% 
    group_by(Plate, Dilution) %>% 
    subset(ID_full == dat1_HCV$ID_full[x])
  #make a dataframe for cvs
  cv_trial<-data.frame(matrix(ncol = 3, nrow =1 ))
        
  #make a loop to find the cv for each deleted point
  for(i in 1:nrow(sing_ID)){
    sing_ID1<-sing_ID[-i,] 
    cv_trial[i]<-cv(sing_ID1$Abs)
  }
        
  # delete the point that gives the lowest cv
  sing_ID<-sing_ID[-which.min(cv_trial),]
  dat_HCV_fixed<-rbind(dat_HCV_fixed,sing_ID)
}


#summarize by sampleID so that we can calculate the mean and std. dev. of the three wells 
dat1_HCV_fixed <- dat_HCV_fixed %>%
  group_by(ID_full) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final),
            IDs=first(IDs), 
            Plate=first(Plate), 
            QAQC_flag = first(QAQC_flag),
            Dilution = first(Dilution), 
            Analysis_rundate = first(Analysis_rundate))

#Flag high cvs 
dat1_HCV_fixed$H2S_cv_flag <- ifelse(dat1_HCV_fixed$H2S_cv > cv_flag_sample, 'High CV', 'Within Range')

# to flag high dilutions 
dat1_HCV_fixed$H2S_flag <- ifelse(dat1_HCV_fixed$H2S_mean <=0, 'bdl', ifelse(dat1_HCV_fixed$H2S_mean > ((mean(std5$Abs)-std_curve$Intercept)/std_curve$Slope)*(dat1_HCV_fixed$Dilution), "adl", "Within Range"))

#recombine data
dat1 <- subset(dat1, (!ID_full %in% dat1_HCV_fixed$ID_full ))
dat1 <- rbind(dat1_HCV_fixed, dat1)
head(dat1)


#plot data and sd's just to check and see what they look like - just a quick first look 
# H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean, color=H2S_flag))+ 
#   geom_point(size=4)+ 
#        scale_color_manual(values = c("adl" = "red", "bdl" = "orange", "Within Range" = "black")) +  theme_classic() + 
#   labs(y="Sulfide (uM)", x="Sample ID",title=paste(Project, Month, Year," Sulfide Data")) + 
#   geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
#                     ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
#   guides(x = guide_axis(angle = 70))
# H2S

#Plot samples after bad reps removed
H2S_fixed <- ggplot(dat1, aes(x=ID_full, y=H2S_mean, color = H2S_cv_flag))+ 
  geom_point(size=4) +  theme_classic() + 
  scale_color_manual(values = c("Within Range" = "darkgreen", "High CV" = "darkred"))+
  labs(y="Sulfide (uM)", x="Sample ID") + 
  facet_wrap(~Plate, scales = "free", ncol = 2) + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
  guides(x = guide_axis(angle = 70)) + 
  ggtitle("Sample triplicate means and sd dev after bad reps removed") 
H2S_fixed

#If samples with cv >10 rerun those samples 
H2S_HighCVRerun <- subset(dat1, H2S_cv_flag == "High CV")
H2S_HighCVRerun$QAQC_flag <- NULL

#filter the samples that need to be run at a higher dilution 
H2S_DiluteRerun <- subset(dat1, H2S_flag == "adl")
ADL<-subset(dat1 ,(ID_full %in% H2S_DiluteRerun$ID_full))
ADL<-filter(ADL,H2S_flag=="Within Range") 
H2S_DiluteRerun<-H2S_DiluteRerun %>% 
  subset((!ID_full %in% ADL$ID_full)) %>% 
  subset((!ID_full %like% " Dup")) %>% 
  subset((!ID_full %like% " Spike"))
#filter the samples that need to be run at a lower dilution 
H2S_bdl <- subset(dat1,H2S_flag == "bdl"& Dilution!=1)
BDL<-subset(dat1 ,(ID_full %in% H2S_bdl$ID_full))
BDL<-filter(BDL,H2S_flag=="Within Range") 
H2S_bdl<-H2S_bdl %>% 
  subset((!ID_full %in% BDL$ID_full)) %>% 
  subset((!ID_full %like% " Dup")) %>% 
  subset((!ID_full %like% " Spike"))

knitr::kable(H2S_HighCVRerun, caption = "High CV Samples")
knitr::kable(H2S_DiluteRerun, caption = "Samples Above the Detection Limit")
knitr::kable(H2S_bdl, caption = "Samples Below the Detection Limit")


```

## Check the dups for QAQC
```{r,echo=FALSE}

#Show me the data that we have from the calculations 
#head(dat1)

#pull out any rows that have "Dup" in the ID column
dups <- dat1 %>%  
  filter(str_detect(ID_full, "Dup")) 
#head(dups)

#remove these from dat1
dat2 <- dat1 %>%  
  filter(!str_detect(ID_full, "Dup")) %>%  
  filter(!str_detect(ID_full, "Spike")) 
#head(dat2)

#remove the dup from these ID_full so we will have duplicates 
dups$ID_full<-gsub(" Dup",'',dups$ID_full)
#head(dups)
colnames(dups) <- c('ID_full', 'mean_dup')


#put it back together with the old data set and look for duplicates 
QAdups <- merge(dat2, dups)
#head(QAdups)

QAdups$dups_chk <- ((abs(QAdups$H2S_mean-QAdups$mean_dup))/((QAdups$H2S_mean+QAdups$mean_dup)/2))*100
QAdups$dups_chk <- ifelse(!is.na(QAdups$dups_chk),QAdups$dups_chk,0 )
QAdups$dups_flag <-  ifelse(QAdups$dups_chk <dups_perc_diff, 'OK', 'Rerun')

#head(QAdups)

#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
dupsbar<-ggplot(data = QAdups, aes(x = ID_full, y = dups_chk, fill=dups_flag)) +
          geom_bar(stat = 'identity') + facet_wrap(~Plate, scales = "free")+
          scale_fill_manual(values=c("OK" ='darkgreen' , "Rerun" = 'darkred')) + 
          theme_classic() + labs(x= "Sample ID", y="Difference Between Duplicates (%)") + 
          theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", color = "black", linewidth=1)

dupsbar

#check for any one's that would warrant reruns 
Baddups <- subset(QAdups, dups_flag == "Rerun")

#write out a flag to the sample dataframe if any dups have percent differences out of range
for(i in 1:length(plates)){
  Baddups1<-  subset(Baddups,Plate %in% plates[i])
  dat$QAQC_flag<-if(nrow(Baddups1) > 0){
    ifelse(dat$Plate %in% qplate$Plate[i], ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, "; Dup perc diff out of range"),"Dup perc diff out of range"),paste0( dat$QAQC_flag,"") )                   
  }else{
    ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, ""),"")
}}

for(i in 1:length(plates)){
  Baddups1<-  subset(Baddups,Plate %in% plates[i])
  dat1$QAQC_flag<-if(nrow(Baddups1) > 0){
    ifelse(dat1$Plate %in% qplate$Plate[i], ifelse(dat1$QAQC_flag != "", paste0(dat1$QAQC_flag, "; Dup perc diff out of range"),"Dup perc diff out of range"),paste0( dat1$QAQC_flag,"") )                   
  }else{
    ifelse(dat1$QAQC_flag != "", paste0(dat1$QAQC_flag, ""),"")
}}

dups_percent <- (sum(QAdups$dups_flag == "OK")/nrow(QAdups))*100
#report out if flags indicate need for rerun
ifelse(dups_percent >= 60  , ">60% of Duplicates are within <10%",
       "<60% of Duplicates are within <10% - REASSESS")

```

## Check the spks for QAQC
```{r,echo=FALSE}

#pull out any rows that have "d" in the SampleID column
spks <- dat1 %>%  filter(str_detect(ID_full, "Spike")) 

#remove the Spike from these ID_full so we will have duplicates 
spks$ID_full<-gsub(" Spike","",spks$ID_full)
#head(spks)
colnames(spks) <- c('ID_full', 'mean_spk')


#put it back together with the old data set and look for duplicates 
QAspks <- merge(dat2, spks)
#head(QAspks)

#now we need to calculate the spike concentration and calculate the spike recovery 
#spike for these samples was 10 uL of the 100uM standard
QAspks$unspkd <- (QAspks$H2S_mean/QAspks$Dilution)*((sample_vol )/Con1) #gives us the total S2- in the sample in umoles
QAspks$spkd <-    (QAspks$mean_spk/QAspks$Dilution)*((sample_vol + spkvol)/Con1)        ##total S2- in spiked sample in umoles 
QAspks$expctd_spkd <-  (QAspks$unspkd + spk_Conc)
QAspks$spk_recovery <-    (QAspks$spkd/QAspks$expctd_spkd)*100
QAspks$spks_flag <-  ifelse(QAspks$spk_recovery > low_recovery_cutoff & QAspks$spk_recovery < high_recovery_cutoff, 'OK', 'NO, rerun')  
 

#head(QAspks)

#plot spk recoveries output as a bar graph to easily check - want any over 10% to be red need to work on this 
spksbar <- ggplot(data = QAspks, aes(x = ID_full, y = spk_recovery, fill=spks_flag)) +facet_wrap(~Plate, scales = "free")+
        geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("OK"="darkgreen","NO, rerun"="darkred")) + 
        theme_classic() + labs(x= "Sample ID", y="Spike Recovery (%)") + 
        theme(legend.position="none") +  
        geom_hline(yintercept=low_recovery_cutoff, linetype="dashed", color = "black", linewidth=1) + 
        geom_hline(yintercept=high_recovery_cutoff, linetype="dashed", color = "black", linewidth=1)

spksbar

#check for any no's that would warrant reruns! 

Badspks <- subset(QAspks, spks_flag == "NO, rerun")

#write out a flag to the sample dataframe if any spks have recovery out of range 
for(i in 1:length(plates)){
  Badspks1<-  subset(Badspks,Plate %in% plates[i])
  dat$QAQC_flag<-if(nrow(Badspks1) > 0){
    ifelse(dat$Plate %in% qplate$Plate[i], ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, "; Spk recovery out of range"),"Spk recovery out of range"),paste0( dat$QAQC_flag,"") )                   
  }else{
    ifelse(dat$QAQC_flag != "", paste0(dat$QAQC_flag, ""),"")
}}

for(i in 1:length(plates)){
  Badspks1<-  subset(Badspks,Plate %in% plates[i])
  dat1$QAQC_flag<-if(nrow(Badspks1) > 0){
    ifelse(dat1$Plate %in% qplate$Plate[i], ifelse(dat1$QAQC_flag != "", paste0(dat1$QAQC_flag, "; Spk recovery out of range"),"Spk recovery out of range"),paste0( dat1$QAQC_flag,"") )                   
  }else{
    ifelse(dat1$QAQC_flag != "", paste0(dat1$QAQC_flag, ""),"")
}}

spks_percent <- (sum(QAspks$spks_flag == "OK")/nrow(QAspks))*100
#report out if flags indicate need for rerun
ifelse(spks_percent >= 60  , ">60% of Spikes are within range",
       "<60% of Spikes are out of range - REASSESS")
```

```{r Format data, include=FALSE}

all_data1 <- dat1 %>%
  rename(
    H2S_Conc_flag = H2S_flag,
    H2S_QAQC_flag = QAQC_flag)

##Remove Dups and Spikes
Sample_data1 <- all_data1 %>% 
  subset((!IDs %like% "Dup")) %>% 
  subset((!IDs %like% "Spike"))

```

```{r Read in and Fix Sample IDs, include = FALSE}

#Read in Sample IDs
SampleIDs1 <- read_excel(SampleIDs1)
SampleIDs1$Number <- as.character(SampleIDs1$Number)
names(SampleIDs1) <- tools::toTitleCase(names(SampleIDs1))

```


```{r Convert Sample Numbers to Sample IDs, include = FALSE}
##Merge with data
Sample_data_IDs1 <- Sample_data1 %>%
  left_join(SampleIDs1, by = c("IDs" = "Number"))

##Make full sample IDs
Sample_data_IDs1 <- Sample_data_IDs1 %>%
  mutate(Sample_ID = paste(Project,
                          Zone, 
                          Source, 
                          Plot,
                          Date, 
                          Depth, 
                          sep = "_"))

##Capitalize all sample IDs
Sample_data_IDs1$Sample_ID <- toupper(Sample_data_IDs1$Sample_ID)

##Remove NAs
Sample_data_IDs1 <- Sample_data_IDs1 %>%  
  filter(!str_detect(Sample_ID, "NA"))

Sample_data_IDs <- Sample_data_IDs1

  
```


```{r Remove ADL/BDL samples that were later diluted differently, include = FALSE}

#filter the high CV data in to one table
HighCV<-filter(Sample_data_IDs, H2S_cv_flag=="High CV")

#filter out the samples that need to be run at a higher dilution
ADL<-filter(Sample_data_IDs, H2S_Conc_flag=="adl")

#remove the samples that have already been run at a higher dilution 
ww_ADL<-subset(Sample_data_IDs, (Sample_ID %in% ADL$Sample_ID))
ww_ADL<-filter(ww_ADL, H2S_Conc_flag=="Within Range") 
ADL_not_fixed<-ADL %>% 
  subset((!Sample_ID %in% ww_ADL$Sample_ID)) %>% 
  subset((!Sample_ID %like% " Dup")) %>% 
  subset((!Sample_ID %like% " Spike"))

#filter out the samples that need to be run at a lower dilution
BDL<-filter(Sample_data_IDs, H2S_Conc_flag=="bdl"& Dilution!=1)

#remove the samples that have already been run at a lower dilution 
ww_BDL<-subset(Sample_data_IDs, (Sample_ID %in% BDL$Sample_ID))
ww_BDL<-filter(ww_BDL,H2S_Conc_flag=="Within Range") 
BDL_not_fixed<-BDL %>% 
  subset((!Sample_ID %in% ww_BDL$Sample_ID)) %>% 
  subset((!Sample_ID %like% " Dup")) %>% 
  subset((!Sample_ID %like% " Spike"))


##Remove samples that were ADL/BDL and are now fixed
ADL_fixed<-ADL %>% 
  subset((Sample_ID %in% ww_ADL$Sample_ID)) %>% 
  subset((!Sample_ID %like% " Dup")) %>% 
  subset((!Sample_ID %like% " Spike"))
ADL_fixed$ID_conc <- paste(ADL_fixed$Sample_ID, ADL_fixed$H2S_Conc_flag, sep = "_")

BDL_fixed<-BDL %>% 
  subset((Sample_ID %in% ww_BDL$Sample_ID)) %>% 
  subset((!Sample_ID %like% " Dup")) %>% 
  subset((!Sample_ID %like% " Spike"))
BDL_fixed$ID_conc <- paste(BDL_fixed$Sample_ID, BDL_fixed$H2S_Conc_flag, sep = "_")


Sample_data_IDs$ID_conc <- paste(Sample_data_IDs$Sample_ID, Sample_data_IDs$H2S_Conc_flag, sep = "_")

Sample_data_wr <- Sample_data_IDs %>% 
  subset(!ID_conc %in% BDL_fixed$ID_conc) %>% 
  subset(!ID_conc %in% ADL_fixed$ID_conc)

##If there are duplicates, remove the one with the lowest CV
Sample_data_nodups <- Sample_data_wr %>%
  arrange(Sample_ID, H2S_cv) %>% # Sort by ID ascending, Value ascending
  distinct(Sample_ID, .keep_all = TRUE) # Keep distinct IDs, retaining all other columns

print(HighCV)
print(ADL_not_fixed)
print(BDL_not_fixed)

```


```{r Pull in active porewater tracking inventory sheet from Google Drive, include=FALSE}

#Run these to pull the TEMPEST porewater metadata into GitHub if not already there
# inventory_directory <- "https://docs.google.com/spreadsheets/d/1sFWq-WKhemPzbOFInqhCu_Lx0lsO6a_Z/edit#gid=496164093"
# 
# drive_download(inventory_directory, overwrite = TRUE)

sheet_names <- excel_sheets("TEMPEST_PorewaterInventory_May2022_Present.xlsx")
sheet_names

raw_metadata_lys <- read_excel("TEMPEST_PorewaterInventory_May2022_Present.xlsx", sheet = "Porewater - Individual", skip = 3)

```

```{r Create similar sample IDs to match with run samples , include=FALSE}

#select SO4/Cl samples 
raw_metadata_lys <- subset(raw_metadata_lys, Analyte == "SO4/Cl/H2S")

#select samples from correct year
raw_metadata_lys_year <- raw_metadata_lys %>%  
  filter(str_detect(Sample_ID, Year))

#separate samples into columns
raw_metadata_lys_year <- raw_metadata_lys_year %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date"),
    remove = FALSE) %>%
  rename(Evacuation_Date = Evacuation_date_YYYMMDD)

raw_metadata_lys_combined <- raw_metadata_lys_year %>%
  dplyr::select("Sample_ID", "Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Evacuation_Date", "Volume_mL", "Notes")


#create IDs from what was collected for comparison later
raw_metadata_lys_combined <- raw_metadata_lys_combined %>%
  mutate(H2S_ID = paste(Project,
                          Zone,
                          Source, 
                          Grid,
                          Collection_Date, 
                          Depth, 
                          sep = "_"))

raw_metadata_lys_combined$H2S_ID <- toupper(raw_metadata_lys_combined$H2S_ID)

#remove old ID's
raw_metadata_lys_combined$Sample_ID <- NULL

sulfide_metadata = raw_metadata_lys_combined

```

```{r Check to see if samples run match metadata & merge info, echo = FALSE}

all_present <- all(Sample_data_nodups$Sample_ID %in% sulfide_metadata$H2S_ID)

if (all_present) {
  message("***All sample IDs are present in metadata.***")
} else {
  message("***Some sample IDs are missing from metadata.***")
  
  # Optional: Which ones are missing?
  missing_ids <- setdiff(Sample_data_nodups$Sample_ID, sulfide_metadata$H2S_ID)
  print(missing_ids)
}

# missing_ids <- data.frame(missing_ids)
# 
# ##Export missing IDs to csv
# write.csv(missing_ids, "20250529_COMPASS_H2S_missing_IDs.csv")

sulfide_metadata$Collection_Date <- as.numeric(sulfide_metadata$Collection_Date)

#merge metadata with sample run data 
merged_data <- Sample_data_nodups %>%
  left_join(sulfide_metadata, by = c("Sample_ID" = "H2S_ID", "Plot" = "Grid", "Date" = "Collection_Date","Project", "Zone", "Source", "Analyte", "Depth"))


```


## Visualize Data by Plot   
```{r Visualize Data, echo=FALSE, warning=FALSE}

#Plot samples to get a first look at concentrations (sanity check)
data_plotting <- merged_data

#Order by Zone from Upland to Surface Water
data_plotting$Zone = factor(data_plotting$Zone, levels = c("C", "FW", "SW"))
data_plotting <- data_plotting[order(data_plotting$Zone), ]

#group the data for plotting
data_plotting <- data_plotting %>%
  mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
  ungroup()

#Plot data and change colors based on Zone:
viz_H2S_plot <- ggplot(data_plotting, aes(x = row_num, y = H2S_mean, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_wrap(~ Zone, scales="free_x") +
  scale_fill_manual(values = c(
    "C" = "springgreen2",
    "FW" = "cyan2",
    "SW" = "violetred2", 
    "ESTUARY" = "grey"
  )) +
  theme_classic() +
  theme(axis.text.x = element_blank()) +
  labs(x = " ", y = "Sulfide (uM)", title = "Samples: Sulfide") +
  scale_x_discrete(drop = TRUE)+
  geom_text(aes(label = ifelse(H2S_mean == 0, "0", "")), vjust = -0.5)+
  theme(
    panel.spacing = unit(.05, "lines"), 
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.1))

viz_H2S_plot

```


```{r Export Combined Data, include = FALSE}

#Prepare data to be exported - if there is anything else to add 
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
  #example read in sample IDs list and merge 
  #create required ID columns in R, etc. 

colnames(merged_data)

final_data <- merged_data %>%
  rename(
    Field_notes = Notes,
    Depth_cm = Depth, 
    Grid = Plot, 
    Collection_Date = Date
    # add more rename pairs as needed
  ) %>%
  mutate(
    Run_notes = run_notes
  ) %>%
  dplyr::select(
    Project, Sample_ID, Zone, Grid, Depth_cm,
         Source, Volume_mL, Collection_Date, Evacuation_Date, 
         H2S_mean, H2S_sd, H2S_cv, H2S_Conc_flag, H2S_cv_flag, H2S_QAQC_flag, Dilution, 
         Analysis_rundate, Field_notes, Run_notes
        # list columns in the order you want them
  )

head(final_data)

#Write out final data frame 
write.csv(final_data, "Processed Data/TEMPEST_H2S_AllPlates_Combined_20250926.csv", row.names = FALSE)

#Write out samples that still have high CVs
write.csv(HighCV, "Processed Data/TEMPEST_H2S_AllPlates_HighCVs_20250926.csv", row.names = FALSE)

#Write out ADL samples
write.csv(ADL_not_fixed, "Processed Data/TEMPEST_H2S_AllPlates_ADL_20250926.csv", row.names = FALSE)

#Write out BDL samples
write.csv(BDL_not_fixed, "Processed Data/TEMPEST_H2S_AllPlates_BDL_20250926.csv", row.names = FALSE)


```

###END