---
title: "COMPASS: TEMPEST Discrete DOC Data QAQC"
author: "TMP Event 2024"
date: "`r Sys.Date()`"
output: pdf_document
---

##Missing sample IDs
```{r}
# Some sample IDs are missing from metadata.
# [1] "FW_I5_DOC_20250611_T1"        "FW_I3_DOC_20240613_T4"        "SW_SOURCEWATER_DOC_202401613"
```

## Run Information
```{r, run info}

#identify which section you are in 
cat("Run Information")

#a link to the Gitbook or whatever protocol you are using for this analysis 
  #steph will add this soon 
  
#anything that needs to be changed do this in the first chunk
  Date_Run = "09/05/24"
  Run_by = "Stephanie J. Wilson"
  Script_run_by = "Stephanie J. Wilson"
  Sample_Year = "2024"
  run_notes = " "
  
  #file path and name for summary file 
    raw_file_name = "tmp_doc_raw_data_2024/TMP_202406_Event.txt" 
  #file path and name for the all peaks file 
    raw_allpeaks_name = "tmp_doc_raw_data_2024/TMP_202406_Event_allpeaks.txt"
  #file path and name for processed data after QAQC
    processed_file_name = "tmp_doc_processed_data_2024/TMP_PW_DOC_Processed_2024_Event.csv"

#check standard concentrations - Update if running different checks: 
   chk_std_c = 1
   chk_std_n = 1
    
#Log path 
    Log_path = "tmp_doc_raw_data_2024/COMPASS_TMP_TOCTN_QAQClog_2024.csv"

```

## Setup
```{r setup, include=FALSE}

#identify which section you are in 
cat("Setup")

#a link to the Gitbook or whatever protocol you are using for this analysis 
  #steph will add this soon 

#Packages that are required 
  library(dplyr)
  library(broom)
  library(ggplot2)
  library(ggpubr)
  library(stringr)
  library(purrr)
  library(tidyverse)
  library(here)
  library(googledrive)

#any coefficients / constants that are needed for calculations 
  mw_c <- 12   #molecular weight of Carbon 
  mw_n <- 14  #molecular weight of Nitrogen
  Con1 <- 1000       # conversion factor value
  Con2 <- 1000000    # conversion factor value 

#Flag that we 
  r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve 
  chk_flag = 0.10   #if the RSD (relative standard deviation) is over 1 among checks this is the std dev. / mean  
  chks_flag = 60    #this is the percent of checks we want to be within range (at least) or else we will flag the data 
  chk_conc_flag = 15 #this is the level cutoff for percent difference of check standards vs. the concentration they are meant to be 
  rep_flag = 25 #this is a 25% error between samples
  #blank_flag is calculated based on samples later in this code
   
#Top standard Concentrations- Update if running different standard curve: 
   top_std_c = 100
   top_std_n = 3
  
#any reference to other code 
   ## if we need the metadata to check sample names we can put path here 

#Set time zone 
  common_tz = "Etc/GMT+5"
  Sys.setenv(TZ = "America/New_York")
  
#plot indicators 
plot_order <- c('Control', 'Freshwater', 'Saltwater')
anyas_colors <- c('springgreen2', 'cyan2', 'violetred2')

```

## Pull in active porewater tracking inventory sheet 
```{r pull in metadata for later, echo=FALSE}

#inventory_directory <- "https://docs.google.com/spreadsheets/d/1sFWq-WKhemPzbOFInqhCu_Lx0lsO6a_Z/edit#gid=496164093"
inventory_directory <- "https://docs.google.com/spreadsheets/d/1sFWq-WKhemPzbOFInqhCu_Lx0lsO6a_Z/edit?gid=496164093#gid=496164093"

directory= file.path(here() %>% dirname(), 'tmp_doc_2024/tmp_doc_metadata_2024')
if(!dir.exists(directory)){
  stop("Directory does not exist, check directory path")
}

#file path for download of the porewater inventory
file_path = file.path(directory, paste0("porewaterinventory_", Sys.Date(), ".xlsx"))

#check if the porewater inventory was downloaded today and if not, donwload it 
if (!file.exists(file_path)) {
  # File does not exist, so download it
  drive_download(inventory_directory, path= file_path, overwrite = FALSE)
} else {
  # File exists, do nothing or print a message
  message("File already exists. No download needed.")
}

#drive_download(inventory_directory, path= file_path, overwrite = FALSE)
```

\newpage

## Import Data Functions  
```{r create function to read in data, echo=FALSE}
## Create a function to read in data from summary file: 

read_data <- function(data){
  # Second, read in data
  read_delim(file = data, skip = 10, delim = "\t", show_col_types = FALSE) %>% 
    rename(sample_name = `Sample Name`, 
           npoc_raw = `Result(NPOC)`, 
           tdn_raw = `Result(TN)`,
           run_datetime = `Date / Time`) %>% 
    select(sample_name, npoc_raw, tdn_raw,run_datetime)
}

read_curve <- function(data){
  # Second, read in data
  read_delim(file = data, skip = 10, delim = "\t", show_col_types = FALSE) %>% 
    rename(sample_name = `Sample Name`,
           analyte = `Analysis(Inj.)`,
           concentration = `Conc.`,
           area = Area,
           manual_dilution = `Manual Dilution`,
           excluded = Excluded,
           run_datetime = `Date / Time`) %>% 
    filter(excluded == 0) %>% #filter to injections that are included in the analysis 
    select(sample_name, analyte, concentration, area, run_datetime) %>%
    pivot_wider(names_from= analyte, values_from = concentration) %>%
    rename(npoc_raw = NPOC,
           tdn_raw = TN)
}
```

## Import Sample Data     
```{r Import Data, echo=FALSE}

cat("Import Sample Data")

#find the file in the raw data folder 
dat_raw <- raw_file_name %>% 
  map_df(read_data) %>% 
  filter(grepl("TMP", sample_name)) %>% # filter to TMP samples only
  bind_rows() 

head(dat_raw)

dat_raw_new <- dat_raw %>%
  mutate(
    # Break into parts
    temp = str_match(sample_name, 
                     "^(.*?)(_T[01])(_AM)?_(\\d{8})$"),
    # Reconstruct in desired order
    sample_name = case_when(
      !is.na(temp[,2]) ~ str_c(temp[,2], "_", temp[,5], temp[,3], temp[,4]),
      TRUE ~ sample_name
    )
  ) %>%
  select(-temp)

head(dat_raw)
```

##Fix incorrectly-entered sample IDs
```{r}

dat_raw$sample_name <- gsub("TMP_FW_I5_T1_20250611", "TMP_FW_I5_T1_20240611", dat_raw$sample_name)
dat_raw$sample_name <- gsub("TMP_FW_I3_T4_AM_20240613", "TMP_FW_I5_T4_AM_20240613", dat_raw$sample_name)
dat_raw$sample_name <- gsub("TMP_SW_SOURCE_Afternoon_202401613", "TMP_SW_SOURCE_Afternoon_20240613", dat_raw$sample_name)

```

\newpage

## Assessing standard Curves 
```{r Assess Standard Curves, echo=FALSE}

cat("Assess the Standard Curve")

#filter standards out of the raw data 
stds_all <- raw_allpeaks_name %>% 
  map_df(read_curve) %>% 
  filter(grepl("CalCurve", sample_name)) %>% 
  dplyr::rename(
        standard_C_ppm = npoc_raw,
        standard_N_ppm = tdn_raw) %>%
  select(run_datetime,standard_C_ppm,standard_N_ppm, area) %>%
  bind_rows() 

#separate by analyte 
stds_C <- stds_all %>%
  filter(!is.na(standard_C_ppm)) %>%
  select(-standard_N_ppm) %>%
  mutate(run_date = as.Date(strptime(run_datetime, format = "%m/%d/%Y %I:%M:%S %p")))

stds_N <- stds_all %>%
  filter(!is.na(standard_N_ppm)) %>%
  select(-standard_C_ppm) %>%
  mutate(run_date = as.Date(strptime(run_datetime, format = "%m/%d/%Y %I:%M:%S %p")))

#calculate slope and r2 of cal curves
#npoc curve
lm_results_c <- stds_C %>%
  group_by(run_date) %>%
  do({
    model = lm(area ~ standard_C_ppm, data = .)
    tidy_model = tidy(model)             # coefficients
    glance_model = glance(model)         # model metrics like R²
    tibble(
      slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
      intercept = tidy_model$estimate[1],
      r2 = glance_model$adj.r.squared
    )
  }) %>%
  mutate(
    analyte = "C", 
    curve = "NPOC (mg/L)"
  )

#tn curve
lm_results_n <- stds_N %>%
  group_by(run_date) %>%
  do({
    model = lm(area ~ standard_N_ppm, data = .)
    tidy_model = tidy(model)             # coefficients
    glance_model = glance(model)         # model metrics like R²
    tibble(
      slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
      intercept = tidy_model$estimate[1],
      r2 = glance_model$adj.r.squared
    )
  }) %>%
  mutate(
    analyte = "N", 
    curve = "TN (mg/L)"
  )

#put the together in one dataframe to later add to log 
Slopes <- rbind(lm_results_c, lm_results_n)

#store the r2's so they plot on the curve graphs
r2_labels_c <- stds_C %>%
  group_by(run_date) %>%
  summarise(
    x_pos = max(standard_C_ppm, na.rm = TRUE) * 0.8,
    y_pos = max(area, na.rm = TRUE),
    r_squared = round(summary(lm(area ~ standard_C_ppm))$adj.r.squared, 4),
    .groups = "drop"
  )

r2_labels_n <- stds_N %>%
  group_by(run_date) %>%
  summarise(
    x_pos = max(standard_N_ppm, na.rm = TRUE) * 0.8,
    y_pos = max(area, na.rm = TRUE),
    r_squared = round(summary(lm(area ~ standard_N_ppm))$adj.r.squared, 4),
    .groups = "drop"
  )

##Plot standard Curve or Curves 
#C Curve
C_stds_plot <- ggplot(stds_C, aes(x = standard_C_ppm, y = area)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  facet_wrap(~ run_date) +
  geom_text(
    data = r2_labels_c,
    aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
    inherit.aes = FALSE,
    hjust = 1, vjust = 1,
    size = 4
  ) +
  labs(
    title = "NPOC Std Curve by Date",
    x = "Carbon Standard Concentration (ppm)",
    y = "Peak Area"
  ) +
  theme_bw()

C_stds_plot

#N Curve
N_stds_plot <- ggplot(stds_N, aes(x = standard_N_ppm, y = area)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "purple") +
  facet_wrap(~ run_date) +
    geom_text(
    data = r2_labels_n,
    aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
    inherit.aes = FALSE,
    hjust = 1, vjust = 1,
    size = 4
  ) +
  labs(
    title = "TN Std Curve by Date",
    x = "Nitrogen Standard Concentration (ppm)",
    y = "Peak Area"
  ) +
  theme_bw()

N_stds_plot

#compare slopes to previous runs (from log) in order to assess drift 
log <- read.csv(Log_path)
log <- log[ ,-c(1)]

#make sure they both have dates as dates 
log$run_date <- as.Date(log$run_date)
log$analyte <- as.character(log$analyte)
log$curve <- as.character(log$curve)
Slopes$run_date <- as.Date(Slopes$run_date)

# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(Slopes, log, by = c("run_date", "analyte"))

# Append the new, non-duplicate rows to log
log <- bind_rows(log, new_rows)

#plot the current slops with teh previous slopes 
Slopes_chk <- ggplot(log, aes(run_date, slope, col=curve)) +
  geom_point(size=4) + 
  geom_line() + 
  theme_bw() + labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
  scale_color_manual(values=c("blue", "purple"))
Slopes_chk

#write out the log file with the added lines for this run  
write.csv(log, Log_path)

#Grab the highest r2 that is available for this run 
r2_C = max(lm_results_c$r2)
r2_N = max(lm_results_n$r2)

#Write out to the user whether or not the r2 is above the cutoff of 0.98
  ifelse(r2_C <= r2_cutoff, 
         "NPOC Curve r2 is below cutoff! - REASSESS", "NPOC Curve r2 GOOD")
  ifelse(r2_N <= r2_cutoff, 
         "TN Curve r2 is below cutoff! - REASSESS", "TN Curve r2 GOOD")
  
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
dat_raw <- dat_raw %>%
  mutate(
    npoc_flag = if (r2_C <= r2_cutoff) {
      "NPOC r2 low"
    } else {
      ""
    },
    tdn_flag = if (r2_N <= r2_cutoff) {
      "TN r2 low"
    } else {
      ""
    }
  )

```

\newpage

## Assess Check Standards 
```{r Check Standards, echo=FALSE}

cat("Assess the Check Standards")

#call to checkstds orrrr Pull out check standards from raw file 
chks_raw <- raw_file_name %>% 
  map_df(read_data) %>% 
  filter(grepl("Chk", sample_name)) %>% # filter to TMP samples only
  bind_rows() 

chks_raw <- chks_raw %>% 
  mutate(rep = row_number())


#RSV of standards 
chks_C_rsv <- ((sd(chks_raw$npoc_raw))/mean(chks_raw$npoc_raw))
chks_N_rsv <- ((sd(chks_raw$tdn_raw))/mean(chks_raw$tdn_raw))

#write out to user about whether or not to continue 
ifelse(chks_C_rsv >= chk_flag, "Carbon CHECK STANDARD RSD TOO HIGH - REASSESS", "Carbon Check Standard RSD within Range")
ifelse(chks_N_rsv >= chk_flag, "Nitrogen CHECK STANDARD RSD TOO HIGH - REASSESS", "Nitrogen Check Standard RSD within Range")

#calculate percent difference between check standards & expected concentration 
#flag if the percent difference is over X% (defined in setup)
#calculate percent difference of check standards 
chks_raw$C_diff <- ((chks_raw$npoc_raw - chk_std_c)/((chks_raw$npoc_raw + chk_std_c)/2)) * 100
chks_raw$C_diff_flag <-  ifelse(abs(chks_raw$C_diff) <= chk_conc_flag, 'YES', 'NO, rerun')

chks_raw$N_diff <- ((chks_raw$tdn_raw - chk_std_n)/((chks_raw$tdn_raw + chk_std_n)/2)) * 100
chks_raw$N_diff_flag <-  ifelse(abs(chks_raw$N_diff) <= chk_conc_flag, 'YES', 'NO, rerun')

#now plot the Ch4 concentrations and the CO2 concentrations vs. the expected concentration 
#then also make the color the percent difference between the expected and observed concentration
c_chks <-  ggplot(data = chks_raw, aes(x = rep, y = npoc_raw, fill=C_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="NPOC (mg/L)", title="Check Stds: NPOC") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=chk_std_c,
              linetype="dashed", color = "black", linewidth=1) + 
              guides(fill=guide_legend(title="% Difference <10%"))


n_chks <-  ggplot(data = chks_raw, aes(x = rep, y = tdn_raw, fill=N_diff_flag)) +
       geom_bar(stat = 'identity') + 
       scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="TN  (mg/L)", title="Check Stds: TN") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=chk_std_n,
              linetype="dashed",  color = "black", linewidth=1) + 
              guides(fill=guide_legend(title="% Difference <10%"))

ggarrange(c_chks, n_chks, nrow=1, ncol=2)

#calculate the percent of check standards that are within the range based on the flag 
c_chks_percent <- (sum(chks_raw$C_diff_flag == "YES")/nrow(chks_raw))*100
n_chks_percent <- (sum(chks_raw$N_diff_flag == "YES")/nrow(chks_raw))*100

#report out if flags indicate need for rerun
ifelse(c_chks_percent >= chks_flag, ">60% of Carbon Check Standards are within range of the expected concentration",
       "<60% of Carbon Check Standards are within range of the expected concentration - REASSESS")
ifelse(n_chks_percent >= chks_flag,">60% of Nitrogen Check Standards are within range of the expected concentration",
       "<60% of Nitrogen Check Standards are within range of the expected concentration - REASSESS")

#write out a flag to the sample dataframe if less than 60% of the checks are within the expected CV
if (c_chks_percent <= chks_flag) {
  dat_raw$npoc_flag <- ifelse(
    dat_raw$npoc_flag != "",
    paste0(dat_raw$npoc_flag, "; NPOC checks out of range"),
    "NPOC checks out of range"
  )
}

if (n_chks_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
  dat_raw$tdn_flag <- ifelse(
    dat_raw$tdn_flag != "",
    paste0(dat_raw$tdn_flag, "; TN checks out of range"),
    "TN checks out of range"
  )
}

```

\newpage

## Assess Blanks 
```{r Check Blanks, echo=FALSE}

cat("Assess Blanks")

#Pull out check standards from raw file 
blks_raw <- raw_file_name %>% 
  map_df(read_data) %>% 
  filter(grepl("DI", sample_name)) %>% # filter to TMP samples only
  bind_rows() 

blks_raw <- blks_raw %>% 
  mutate(rep = row_number())


#I think we want to check that the blank concentrations are less than the lowest 25% of sample concentrations: 

blk_flag_c <- quantile(dat_raw$npoc_raw, prob=c(.25))   #this gives you the lower 25% quartile of the data 
blks_raw$C_diff_flag <-  ifelse(blks_raw$npoc_raw <= blk_flag_c, 'YES', 'NO, rerun')

blk_flag_n <- quantile(dat_raw$tdn_raw, prob=c(.25))   #this gives you the lower 25% quartile of the data 
blks_raw$N_diff_flag <-  ifelse(blks_raw$tdn_raw <= blk_flag_n, 'YES', 'NO, rerun')

#calculate the percent of check standards that are within the range based on the flag 
c_blks_percent <- (sum(blks_raw$C_diff_flag == "YES")/nrow(blks_raw))*100
n_blks_percent <- (sum(blks_raw$N_diff_flag == "YES")/nrow(blks_raw))*100

#report out if flags indicate need for rerun
ifelse(c_blks_percent >= chks_flag, ">60% of Carbon Blank concentrations are below the lower 25% quartile of samples",
       "<60% of Carbon blanks are higher than the lower 25% quartile of samples - REASSESS")
ifelse(n_blks_percent >= chks_flag, ">60% of Nitrogen Blank concentrations are below the lower 25% quartile of samples",
       "<60% of Nitrogen blanks are higher than the lower 25% quartile of samples - REASSESS")


#now plot the DOC concentrations and the TDN concentrations vs. the expected concentration 
#then also make the color the percent difference between the expected and observed concentration
c_blks <-  ggplot(data = blks_raw, aes(x = rep, y = npoc_raw, fill=C_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkblue", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="NPOC  (mg/L)", title="Check Stds: NPOC") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=blk_flag_c, linetype="dashed", 
                color = "black", linewidth=1)  + 
                guides(fill=guide_legend(title="Blank Conc <25% Quartile Samples"))


n_blks <-  ggplot(data = blks_raw, aes(x = rep, y = tdn_raw, fill=N_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkblue", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="TN  (mg/L)", title="Check Stds: TN") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=blk_flag_n, linetype="dashed", 
                color = "black", linewidth=1)  + 
                guides(fill=guide_legend(title="Blank Conc <25% Quartile Samples"))

ggarrange(c_blks, n_blks, nrow=1, ncol=2)

#find average of run carbon and nitrogen blanks for flagging samples later 
blk_avg_c <- mean(blks_raw$npoc_raw)

cat("carbon blanks:")
print(blk_avg_c)

blk_avg_n <- mean(blks_raw$tdn_raw)

cat("nitrogen blanks:")
print(blk_avg_n)

#write out a flag to the sample dataframe if more than 60% of the blanks are above the lower 25% quantile of samples
if (c_blks_percent <= chks_flag) {
  dat_raw$npoc_flag <- ifelse(
    dat_raw$npoc_flag != "",
    paste0(dat_raw$npoc_flag, "; NPOC blanks out of range"),
    "NPOC blanks out of range"
  )
}

if (n_blks_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
  dat_raw$tdn_flag <- ifelse(
    dat_raw$tdn_flag != "",
    paste0(dat_raw$tdn_flag, "; TN blanks out of range"),
    "TN blanks out of range"
  )
}

```

\newpage

## Assess Duplicates - if there are any 
```{r Check Duplicates, echo=FALSE}

cat("Assess Duplicates")

#Take a look at the raw data 
  #head(dat_raw)

#pull out any rows that have "dup" in the sample_name column
dups <- dat_raw %>%  
  select(!c(npoc_flag, tdn_flag)) %>%
  filter(str_detect(sample_name, "dup"))      #have to change this to match data

#create a new dataframe and remove dups from sample dataframe 
dat_raw2 <- dat_raw %>%  
  filter(!str_detect(sample_name, "dup")) 

#remove the dup from these IDs so we will have duplicate sample names
dups$sample_name<-gsub("_dup","",as.character(dups$sample_name))
dups <- dups[ ,-c(4)] #remove the run date time for 
colnames(dups) <- c('sample_name', 'npoc_raw_dup', "tdn_raw_dup")
head(dups)

QAdups <- merge(dat_raw2, dups)
head(QAdups)

df2 <- as.data.frame(QAdups$npoc_raw)
df2$dups <- QAdups$npoc_raw_dup

df2$sds <- apply(df2,1,sd)
df2$mean <- apply(df2, 1, mean)

QAdups$npoc_dups_cv <- (df2$sds/df2$mean) * 100
QAdups$npoc_dups_cv_flag <-  ifelse(QAdups$npoc_dups_cv <10, 'YES', 'NO, rerun')

df3 <- as.data.frame(QAdups$tdn_raw)
df3$dups <- QAdups$tdn_raw_dup

df3$sds <- apply(df3,1,sd)
df3$mean <- apply(df3, 1, mean)

QAdups$tdn_dups_cv <- (df3$sds/df3$mean) * 100
QAdups$tdn_dups_cv_flag <-  ifelse(QAdups$tdn_dups_cv <10, 'YES', 'NO, rerun')

head(QAdups)

#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
C_dups <- ggplot(data =QAdups, aes(x =sample_name, y =npoc_dups_cv, fill=npoc_dups_cv_flag)) +
       geom_bar(stat = 'identity') + 
        theme_classic() + labs(x= "Sample ID", y="CV of NPOC Dups (%)") + 
        scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "red")) +
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1)  + 
              guides(fill=guide_legend(title="CV Between Dups <10%")) +
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))


N_dups <- ggplot(data =QAdups, aes(x =sample_name, y =tdn_dups_cv, fill=tdn_dups_cv_flag)) +
       geom_bar(stat = 'identity') + 
        theme_classic() + labs(x= "Sample ID", y="CV of TN Dups (%)") + 
          scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "red")) +
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1) + 
              guides(fill=guide_legend(title="CV Between Dups <10%"))+
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))

ggarrange(C_dups, N_dups,ncol=2, nrow=1)


#calculate the percent of check standards that are within the range based on the flag 
c_dups_percent <- (sum(QAdups$npoc_dups_cv_flag == "YES")/nrow(QAdups))*100
n_dups_percent <- (sum(QAdups$tdn_dups_cv_flag == "YES")/nrow(QAdups))*100

#report out if flags indicate need for rerun
ifelse(c_dups_percent >= chks_flag, ">60% of Carbon Duplicates have a CV <10%",
       "<60% of Carbon Duplicates have a CB <10% - REASSESS")
ifelse(n_dups_percent >= chks_flag, ">60% of Nitrogen Duplicates have a CV <10%",
       "<60% of Nitrogen Duplicates have a CB <10% - REASSESS")

#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range 
# if (c_dups_percent <= chks_flag) {
#   dat_raw$npoc_flag <- ifelse(
#     dat_raw$npoc_flag != "",
#     paste0(dat_raw$npoc_flag, "; NPOC dups out of range"),
#     "NPOC dups out of range"
#   )
# }
# 
# if (n_dups_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
#   dat_raw$tdn_flag <- ifelse(
#     dat_raw$tdn_flag != "",
#     paste0(dat_raw$tdn_flag, "; TN dups out of range"),
#     "TN dups out of range"
#   )
# }

```

\newpage

## Sample Flagging   
```{r Sample Flagging, echo=FALSE}

cat("Sample Flagging")

#Flagging data if the concentration is outside the standards range and based on blanks
dat_flagged <- dat_raw %>%
  mutate(
    npoc_flag = if_else(
      npoc_raw > top_std_c,
      if_else(
        npoc_flag != "" & !is.na(npoc_flag),
        paste0(npoc_flag, "; value above cal curve"),
        "value above cal curve"
      ),
      npoc_flag
    ),
    npoc_flag = if_else(
      blk_avg_c > 0.25 * npoc_raw,
      if_else(
        npoc_flag != "" & !is.na(npoc_flag),
        paste0(npoc_flag, "; blank is ≥ 25% of sample value"),
        "blank is ≥ 25% of sample value"
      ),
      npoc_flag
    ),

    tdn_flag = if_else(
      tdn_raw > top_std_n,
      if_else(
        tdn_flag != "" & !is.na(tdn_flag),
        paste0(tdn_flag, "; value above cal curve"),
        "value above cal curve"
      ),
      tdn_flag
    ),
    tdn_flag = if_else(
      blk_avg_n > 0.25 * tdn_raw,
      if_else(
        tdn_flag != "" & !is.na(tdn_flag),
        paste0(tdn_flag, "; blank is ≥ 25% of sample value"),
        "blank is ≥ 25% of sample value"
      ),
      tdn_flag
    )
  )

#lets make a dataframe with just the concentration flag for plotting 
dat_flag_viz <- dat_raw %>% 
  mutate(npoc_flag = case_when(npoc_raw > top_std_c ~ "value above cal curve",
            blk_avg_c > 0.25*npoc_raw ~ "blank is ≥ 25% of sample value"), # flagging if blank concentration is > 20% of the sample concentration
            #sample_name == "TMP_SW_F4_T3" ~ "incorrect sample naming, cannot resolve"), # if needed and there are issues with sample names
    
        tdn_flag = case_when(tdn_raw > top_std_n ~ "value above cal curve",
            blk_avg_n > 0.25*tdn_raw ~ "blank is ≥ 25% of sample value") #, # flagging if blank concentration is > 25% of the sample concentration
          #sample_name == "TMP_SW_F4_T3" ~ "incorrect sample naming, cannot resolve"), # if needed and there are issues with sample names
 )

dat_flag_viz <- dat_flag_viz %>% 
  mutate(rep = row_number())

#Plot data and change colors based on flags to check it: 
c_samples_flag <-  ggplot(data = dat_flag_viz, aes(x = sample_name, y = npoc_raw, fill=npoc_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("red", "orange"))+
        theme_classic() + labs(x= " ", y="C (mg/L)", title="C: Grey = Within Range of Curve") + 
        theme(legend.position="none")  +
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))


n_samples_flag <-  ggplot(data = dat_flag_viz, aes(x = sample_name, y = tdn_raw, fill=tdn_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("red", "orange"))+
        theme_classic() + labs(x= " ", y="N (mg/L)", title="N: Grey = Within Range of Curve") + 
        theme(legend.position="none")  +
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))

#ggarrange(c_samples_flag, n_samples_flag, nrow=1, ncol=2)
print(c_samples_flag)
print(n_samples_flag)

```

\newpage

## Visualize Data by Plot   
```{r Visualize Data, echo=FALSE}

cat("Visualize Data")

#Plot samples to get a first look at concentrations (sanity check)
IDs <- data.frame(do.call('rbind', strsplit(as.character(dat_flagged$sample_name),'_',fixed=TRUE)))
colnames(IDs) <- c("Site_Code" , "Plot", "Grid_Square",  "Extra", "Extra1", "Extra2")
head(IDs)

#rejoin them to the dataframe
dat_florplot <- cbind(IDs, dat_flagged)
head(dat_florplot)

dat_florplot <- dat_florplot %>% 
  mutate(Plot_name = case_when(Plot == 'SW' ~ "Saltwater", 
                               Plot == 'FW' ~ "Freshwater", 
                               Plot == 'C' ~ "Control"))


#Plot data and change colors based on flags to check it: 
viz_c_plot <-  ggplot(data = dat_florplot, aes(x = Grid_Square, y = npoc_raw, fill=Plot_name)) +
       geom_bar(stat = 'identity') + 
        facet_grid(~Plot_name, scales="free_x") + 
        #facet_grid(~factor(Plot_name, levels=c('Control', 'Freshwater', 'Saltwater'))) + 
        scale_fill_manual(values = c("Control" = "springgreen2", "Freshwater" = "cyan2",
                                      "Saltwater" = "violetred2")) +
        theme_classic() + labs(x= " ", y="NPOC (mg/L)", title="Carbon by Plot") + 
        theme(legend.position="none") 


viz_n_plot <-  ggplot(data = dat_florplot, aes(x = Grid_Square, y = tdn_raw, fill=Plot_name)) +
       geom_bar(stat = 'identity') + 
        facet_grid(~Plot_name, scales="free_x") +
        #facet_grid(~factor(Plot_name, levels=c('Control', 'Freshwater', 'Saltwater'))) + 
        scale_fill_manual(values = c("Control" = "springgreen2", "Freshwater" = "cyan2",
                                      "Saltwater" = "violetred2")) + 
        theme_classic() + labs(x= " ", y="TN (mg/L)", title="Nitrogen by Plot") + 
        theme(legend.position="none") 

#ggarrange(viz_c_plot, viz_n_plot, nrow=1, ncol=2)
print(viz_c_plot)
print(viz_n_plot)

```

\newpage

## Convert data from mg/L to uMoles/L 
```{r, Unit Conversion, include=FALSE}

#convert npoc and tdn from mg/L to uMoles/L 
dat_flagged <- dat_flagged %>%
  mutate(
    npoc_uM = (((as.numeric(dat_flagged$npoc_raw))/Con1)/mw_c)*Con2, 
    tdn_uM = (((as.numeric(dat_flagged$tdn_raw))/Con1)/mw_n)*Con2

```

##Make IDs to match the metadata
```{r}

#select event samples without AM/PM
all_dat_event <- dat_flagged %>%  
  filter(str_detect(sample_name, "_T"))%>%  
  filter(!str_detect(sample_name, "M_"))

#separate event samples into columns TMP_FW_C6_T0_20240610
all_dat_event <- all_dat_event %>%
  separate(
    col = sample_name,
    sep = "_",
    into = c("Project", "Plot", "Grid", "Event_Time", "Collection_Date"),
    remove = FALSE)

#select event samples with AM/PM
all_dat_event_time <- dat_flagged %>%  
  filter(str_detect(sample_name, "_T"))%>%  
  filter(str_detect(sample_name, "M_"))

#separate event samples into columns TMP_SW_B4_T2_AM_20240612
all_dat_event_time <- all_dat_event_time %>%
  separate(
    col = sample_name,
    sep = "_",
    into = c("Project", "Plot", "Grid", "Event_Time", "Time_of_day", "Collection_Date"),
    remove = FALSE)

#select non-event samples 
all_dat_monmon <- dat_flagged %>%  
  filter(!str_detect(sample_name, "_T"))%>%  
  filter(!str_detect(sample_name, "SOURCE"))%>%  
  filter(!str_detect(sample_name, "ESTUARY"))

#separate non-event samples into columns TMP_C_H3_20240607
all_dat_monmon <- all_dat_monmon %>%
  separate(
    col = sample_name,
    sep = "_",
    into = c("Project", "Plot", "Grid", "Collection_Date"),
    remove = FALSE)

#select source samples 
all_dat_source <- dat_flagged %>%  
  filter(str_detect(sample_name, "SOURCE"))

#separate non-event samples into columns TMP_SW_SOURCE_Morning_20240611
all_dat_source <- all_dat_source %>%
  separate(
    col = sample_name,
    sep = "_",
    into = c("Project", "Plot", "Grid", "Time_of_day", "Collection_Date"),
    remove = FALSE)

#select tidal samples 
all_dat_tides <- dat_flagged %>%  
  filter(str_detect(sample_name, "T_"))

#separate tidal samples into columns TMP_ESTUARY_LT_20240611
all_dat_tides <- all_dat_tides %>%
  separate(
    col = sample_name,
    sep = "_",
    into = c("Project", "Plot", "Tide", "Collection_Date"),
    remove = FALSE)

#combine event and non-event samples 
all_dat_pw <- bind_rows(all_dat_event, all_dat_event_time, all_dat_monmon)
all_dat_pw$Type <- "Porewater"

#combine source samples
all_dat_sw <- bind_rows(all_dat_source, all_dat_tides)
all_dat_sw$Grid <- "SourceWater"

#combine all samples
all_dat <- bind_rows(all_dat_pw, all_dat_sw)
all_dat$Depth = "15cm"


# TMP_FW_C3_20240614_15cm_T6
all_dat <- all_dat %>%
  mutate(sample_ID = paste(Plot, 
                          Grid,
                          "DOC", 
                          Collection_Date, 
                          Event_Time, 
                          sep = "_"))

all_dat$sample_ID <- gsub("_NA", "", all_dat$sample_ID)
```


##Create similar sample IDs to match with run samples 
```{r pull in metadata for later, include=FALSE}

raw_metadata_lys <- read_excel("TEMPEST_PorewaterInventory_May2022_Present.xlsx", sheet = "Porewater - Individual", skip = 3, col_types = "text")
raw_metadata_sw <- read_excel("TEMPEST_PorewaterInventory_May2022_Present.xlsx", sheet = "Source Water", skip = 3, col_types = "text")

#select DOC samples 
raw_metadata_lys <- subset(raw_metadata_lys, Analyte == "DOC")

#select 2024 samples
raw_metadata_lys_year <- raw_metadata_lys %>%  
  filter(str_detect(Sample_ID, Sample_Year))

#select event samples 
raw_metadata_lys_event <- raw_metadata_lys_year %>%  
  filter(str_detect(Sample_ID, "_T"))

#separate event samples into columns
raw_metadata_lys_event <- raw_metadata_lys_event %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Event_Time", "Collection_Date", "Time_of_day"),
    remove = FALSE)

#select non-event samples 
raw_metadata_lys_monmon <- raw_metadata_lys_year %>%  
  filter(!str_detect(Sample_ID, "_T"))

#separate non-event samples into columns
raw_metadata_lys_monmon <- raw_metadata_lys_monmon %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Event_Time", "Time_of_day"),
    remove = FALSE)

#combine event and non-event samples 
raw_metadata_lys_combined <- rbind(raw_metadata_lys_event, raw_metadata_lys_monmon)
raw_metadata_lys_combined <- raw_metadata_lys_combined %>%
  select("Sample_ID", "Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", "Evacuation_date_YYYMMDD", 
         "Collection_Start_Time_24hrs", "Collection_End_Time_24hrs", "Collection_Date_YYYYMMDD", "EST_EDT", 
         "Event_Time", "Time_of_day", "Volume_mL", "Notes")
colnames(raw_metadata_lys_event)

#create IDs from what was collected for comparison later
raw_metadata_lys_combined <- raw_metadata_lys_combined %>%
  mutate(DOC_ID = paste(Zone,
                        Grid,
                        "DOC", 
                        Collection_Date, 
                        Event_Time, 
                        sep = "_"))

raw_metadata_lys_combined$DOC_ID <- gsub("_NA", "", raw_metadata_lys_combined$DOC_ID)




#select DOC samples 
raw_metadata_sw <- raw_metadata_sw %>%  
  filter(str_detect(Sample_ID, "DOC"))

#select 2024 samples
raw_metadata_sw_year <- raw_metadata_sw %>%  
  filter(str_detect(Sample_ID, Sample_Year))

#select rainwater samples 
raw_metadata_sw_rw <- raw_metadata_sw_year %>%  
  filter(str_detect(Sample_ID, "_Rainwater"))

#separate event samples into columns
raw_metadata_sw_rw <- raw_metadata_sw_rw %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", NA, "Source", "Zone", "Analyte", "Collection_Date", "Grid", "Depth", "Event_Time", "Time_of_day"),
    remove = FALSE)

#select sourcewater samples 
raw_metadata_sw_sw <- raw_metadata_sw_year %>%  
  filter(!str_detect(Sample_ID, "_Rainwater"))

#separate samples into columns
raw_metadata_sw_sw <- raw_metadata_sw_sw %>%
  separate(
    col = Sample_ID,
    sep = "_",
    into = c("Project", "Source", "Zone", "Analyte", "Collection_Date", "Time_of_day", "Grid", "Depth", "Event_Time"),
    remove = FALSE)

#combine rw and sw samples 
raw_metadata_sw_combined <- rbind(raw_metadata_sw_rw, raw_metadata_sw_sw)
raw_metadata_sw_combined <- raw_metadata_sw_combined %>%
  rename("Notes" = "Notes:") %>%
  select("Sample_ID", "Project", "Zone", "Source" ,"Grid", "Depth", "Analyte", "Collection_Date", 
         "Deployment_Date_YYYYMMDD", "Deployment_Time_24hrs", "Collection_Time_24hrs", "Collection_Date_YYYYMMDD", "EST/EDT", 
         "Event_Time", "Time_of_day", "Volume_mL", "Notes")
colnames(raw_metadata_sw_rw)

#change "Source" to "Sourcewater" 
raw_metadata_sw_combined$Source <- replace(raw_metadata_sw_combined$Source, raw_metadata_sw_combined$Source == "Source", "SourceWater")

#create IDs from what was collected for comparison later
raw_metadata_sw_combined <- raw_metadata_sw_combined %>%
  rename(EST_EDT = `EST/EDT`) %>%
  mutate(DOC_ID = paste(Zone,
                          Source, 
                        "DOC", 
                          Collection_Date, 
                          sep = "_"))


#combine porewater and sourcewater samples
doc_metadata <- bind_rows(raw_metadata_lys_combined, raw_metadata_sw_combined)

#remove old ID's
doc_metadata$Sample_ID <- NULL

# head(doc_metadata)

```

## Check to see if samples run match metadata & merge info
```{r check sample ids with metadata, echo=FALSE, warning = FALSE}

#check to see if all samples are present in the metadata 
all_dat$sample_ID <- toupper(all_dat$sample_ID)
doc_metadata$DOC_ID <- toupper(doc_metadata$DOC_ID)

all_present <- all(all_dat$sample_ID %in% doc_metadata$DOC_ID)

if (all_present) {
  message("All sample IDs are present in metadata.")
} else {
  message("Some sample IDs are missing from metadata.")
  
  # Optional: Which ones are missing?
  missing_ids <- setdiff(all_dat$sample_ID, doc_metadata$DOC_ID)
  print(missing_ids)
}


all_dat <- all_dat %>%
  select(sample_ID, npoc_raw, tdn_raw, run_datetime, npoc_flag, tdn_flag, npoc_uM, tdn_uM)

# colnames(all_dat)
# colnames(doc_metadata)

#merge metadata with sample run data 
merged_data <- all_dat %>%
  left_join(doc_metadata, by = c("sample_ID" = "DOC_ID"))


```


## Add in/check metadata 
```{#r, check sample ids with metadata, echo=FALSE}

cat("Check Sample IDs with Metadata")

sample_collected_metadata <- readxl::read_excel(file_path, skip=3, sheet="Porewater - Individual") %>%
  select(Project, Plot, Vial_ID, Volume_mL, Evacuation_date_YYYMMDD, Collection_Date_YYYYMMDD, 
         Collection_Start_Time_24hrs, Collection_End_Time_24hrs, EST_EDT) %>%
  mutate(sample_type = stringr::str_extract(Vial_ID, 'SO4|FE|DOC|NUTR|AMINO|ISO|SAC|HR-MS|CDOM')) %>% #get the sample type from the Vial_ID
  filter(sample_type == "DOC") #filter to just DOC samples

samples_collected <- sample_collected_metadata %>%
  #note sample names are not entered in the sheets the same way: DOC data entered time_plot_grid; metadata entered plot_grid_date
  mutate(plot = stringr::str_extract(Vial_ID, 'SW|FW|C'),
         grid = stringr::str_extract(Vial_ID, "B4|C3|C6|D5|E3|F4|F6|H3|H6|I5") ,
         date = stringr::str_extract(Vial_ID, "[0-9]{8}"),
         sample_name = paste0("TMP", "_", plot, "_", grid, "_", date )) 

# Clean up the data to make sure it will match the metadata file 
dat_flagged <- dat_flagged %>%
  mutate(sample_name = str_replace(sample_name, "CTRL", "C")) %>%
  # Remove rows where sample_name ends with "_dup"
  filter(!str_detect(sample_name, "_dup$"))

#check if metadata was recorded for each of the samples run 
dat_flagged %>%
  mutate(metadata_recorded = case_when(
    sample_name %in% samples_collected$sample_name ~ TRUE,
    TRUE ~ FALSE
  )) %>%
  select(sample_name, metadata_recorded) %>%
  print()

#join in the metadata to the data 
merged_data <- dat_flagged %>%
  left_join(samples_collected, by = "sample_name")

```

## Export Processed Data  
```{r, Export Processed Data, echo=FALSE}

cat("Export Processed Data")

#Prepare data to be exported - if there is anything else to add 
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
  #example read in sample IDs list and merge 
  #create required ID columns in R, etc. 
final_data <- merged_data %>% 
  #select(Project, Plot, grid, sample_name, Vial_ID, date, ) %>%
  mutate(
    Project = "COMPASS: TEMPEST",   # new column with same value on every row
    Depth_cm = 15,
    Run_notes = run_notes,  # new column with notes about the run
    sample_type = "DOC", 
  ) 

#this needs altered to match the tempest metadata and clean up 
colnames(final_data)
final_data <- final_data %>%
  rename(
    npoc_mgL = npoc_raw, 
    tdn_mgL = tdn_raw,
    Analysis_runtime = run_datetime, 
    plot = Plot, 
    grid = Grid, 
    Vial_ID = sample_ID, 
    date = Collection_Date, 
    Field_notes = Notes
    # add more rename pairs as needed
  ) %>%
  select(
    Project, plot, grid, Depth_cm, sample_type, Vial_ID, date, 
    npoc_mgL, npoc_uM, npoc_flag, tdn_mgL, tdn_uM, tdn_flag, Analysis_runtime,
    Run_notes, Field_notes, Evacuation_date_YYYMMDD, Collection_Date_YYYYMMDD, 
    Collection_Start_Time_24hrs, Collection_End_Time_24hrs, EST_EDT, Volume_mL
    # list columns in the order you want them
  )

head(final_data)

#will put final data in processed data folder 
  write.csv(final_data, processed_file_name)


```

#end
